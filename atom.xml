<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Convex Optimization</title>
 <link href="/atom.xml" rel="self"/>
 <link href="/"/>
 <updated>2017-01-29T20:20:09-08:00</updated>
 <id></id>
 <author>
   <name>Akshay Agrawal</name>
   <email></email>
 </author>

 
 <entry>
   <title>First results for alternating projection acceleration</title>
   <link href="/2016/12/07/ap-first-results/"/>
   <updated>2016-12-07T00:00:00-08:00</updated>
   <id>/2016/12/07/ap-first-results</id>
   <content type="html">&lt;ol&gt;
  &lt;li&gt;Plane search does not obey fejer monotonicity&lt;/li&gt;
  &lt;li&gt;Surprisingly, initial evidence implies that the heavy ball method
only hurts. This is of course not a conclusion, but rather an anecdotal
observation.&lt;/li&gt;
  &lt;li&gt;QP (mem == 2) + overprojection seems to work decently well. Is the QP
projection cheap? Probably. Closed form? Maybe.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-baseline-algorithms&quot;&gt;The baseline algorithms:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Alternating projections&lt;/li&gt;
  &lt;li&gt;Alternating projections with heavy-ball momentum&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;the-state-of-the-art&quot;&gt;The state-of-the-art:&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Dykstra&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-did-i-try&quot;&gt;What did I try?&lt;/h2&gt;
&lt;p&gt;Problem set-up:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1500-dimensional variable&lt;/li&gt;
  &lt;li&gt;Affine constraint with random matrix &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; \in &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{1000 \times 1500}&lt;/script&gt;,
1% sparsity.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
x = \begin{bmatrix}y &amp; z\end{bmatrix} %]]&gt;&lt;/script&gt;,
&lt;script type=&quot;math/tex&quot;&gt;y \in \mathbb{R}^{500}, z&lt;/script&gt; constrained to be in the second-order cone.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(1) A &lt;strong&gt;plane search&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\optmin{\theta}{\dist{\sum_{i} \theta_i a_i}{\mathcal{C}}^2}{\sum_{i} \theta_i = 1, \quad \theta_i \in [0, 1],}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;a_i&lt;/script&gt; are a finite number of previous iterates that were on the affine set,
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; the convex cone. It turns out that performing this plane search produces
sequences that are &lt;strong&gt;not Fejer monotonic&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;(2) &lt;strong&gt;Halfpsace Outer Approximation (HOA)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I used the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; most recent halfspaces generated by projecting onto the convex
sets, and obtained the next iterate by projecting onto the intersection of these
sets. This works “fairly well.” It works “better” than Dykstra’s if we only look
at the number of iterations. Of course, the number of iterations isn’t a “good”
metric, but it’s a start. Even small &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;k = 2&lt;/script&gt;) works well.&lt;/p&gt;

&lt;p&gt;(3) &lt;strong&gt;HOA with randomly accepting new hyperplanes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I don’t understand this well yet, but I suspect there is something interesting
here.&lt;/p&gt;

&lt;p&gt;(4) &lt;strong&gt;HOA with momentum&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Anecdotally, momentum “helps” with respect to the number of iterations.
Over-projecting seems to be more useful than interpolating between the previous
velocity and the current one. Again, very anecdotal evidence.&lt;/p&gt;

&lt;h2 id=&quot;future-work-and-experiments&quot;&gt;Future work and experiments&lt;/h2&gt;
&lt;p&gt;(0) Write-up what I’ve done and learned this quarter and send it to Prof. Boyd.&lt;/p&gt;

&lt;p&gt;(1) Anneal the momentum (say by a factor of &lt;script type=&quot;math/tex&quot;&gt;1/\sqrt{n}&lt;/script&gt;,
    &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; the # of iterations)?&lt;/p&gt;

&lt;p&gt;(2) Flip a p-biased coin to decide whether or not to oveproject&lt;/p&gt;

&lt;p&gt;(3) Flip a p-biased coin and overproject by a lot&lt;/p&gt;

&lt;p&gt;(4) Take advantage of short-and-fat matrix structure, if it arises&lt;/p&gt;

&lt;p&gt;(5) Project onto random subsets of equality constraints&lt;/p&gt;

&lt;p&gt;(6) Test on the homogeneous self-dual embedding of a cone program (SCS).&lt;/p&gt;

&lt;h2 id=&quot;questions-for-professor-boyd&quot;&gt;Questions for Professor Boyd&lt;/h2&gt;
&lt;p&gt;(1) Does a closed-form projector onto the intersection of two halfspaces
    exist, and/or can it be computed cheaply by some means?&lt;/p&gt;

&lt;p&gt;(2) Why is the optimal value returned by CVXPY inaccurate?
   (roughly 10e-2 precision.)&lt;/p&gt;

&lt;p&gt;(3) Lots of stuff I want to try empirically. If anything stands out,
    will go forward with theoretical analysis. HOA + momentum seems the
    most promising right now. Does that sound good?&lt;/p&gt;

&lt;p&gt;(4) Logistics for continuing the work next quarter? Is an independent study
    still possible? I think I have sufficient momentum now to carry me forward.
    Will you be reachable by email? (If not, that’s totally understandable!)&lt;/p&gt;

&lt;p&gt;(5) I’m still very much interested in TA-ing convex in the spring, if the offer
    still stands.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Checklist</title>
   <link href="/2016/11/09/visual-checklist/"/>
   <updated>2016-11-09T00:00:00-08:00</updated>
   <id>/2016/11/09/visual-checklist</id>
   <content type="html">&lt;h2 id=&quot;the-problems&quot;&gt;1. The Problems.&lt;/h2&gt;
&lt;p&gt;Each problem must eventually be cast as a feasibility problem. In particular,
the feasibility problem must be cast as finding a point in the intersection
of an affine set and a cone. Note that the cone, however, may be the
cross-product of multiple different cones.&lt;/p&gt;

&lt;h2 id=&quot;the-algorithms&quot;&gt;2. + 3. The Algorithms&lt;/h2&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\text{Specification}]}\color{blue}{[\text{Implementation}]}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\checkmark]}\color{blue}{[\checkmark]}&lt;/script&gt; Alternating projections&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\checkmark]}\color{blue}{[\checkmark]}&lt;/script&gt; Dykstra’s&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\checkmark]}\color{blue}{[\circ]}&lt;/script&gt;  ADMM&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\checkmark]}\color{blue}{[\checkmark]}&lt;/script&gt;
  Halfspace Eliminating Alternating Projections (memory &lt;script type=&quot;math/tex&quot;&gt;k = 2&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\checkmark]}\color{blue}{[\checkmark]}&lt;/script&gt;
  Halfspace Eliminating Alternating Projections (memory &lt;script type=&quot;math/tex&quot;&gt;k &gt; 2&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\checkmark]}\color{blue}{[\checkmark]}&lt;/script&gt;
  Halfspace Eliminating Alternating Projections, with momentum&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\circ]}\color{blue}{[\circ]}&lt;/script&gt;
  Halfspace Eliminating Alternating Projections, with plane search / momentum&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\color{red}{[\circ]}\color{blue}{[\circ]}&lt;/script&gt;
  Halfspace Eliminating Alternating Projections, with plane search / momentum
  and additional problem transformations&lt;/p&gt;

&lt;h2 id=&quot;theoretical-analyses&quot;&gt;4. Theoretical Analyses&lt;/h2&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;{[\checkmark]}&lt;/script&gt;
  Halfspace Eliminating Alternating Projections (free, from Pang)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;{[\circ]}&lt;/script&gt; Halfspace Eliminating Alternating Projections, with plane search&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;{[\circ]}&lt;/script&gt; Halfspace Eliminating Alternating Projections, with plane search
  and additional problem transformations&lt;/p&gt;

&lt;h2 id=&quot;the-test-cases&quot;&gt;5. The Test Cases&lt;/h2&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;[\checkmark]&lt;/script&gt; 2D feasibility problems, for intuition’s sake.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;[\checkmark]&lt;/script&gt; Intersection of second-order cone and affine set,
  randomly generated&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;[\circ]&lt;/script&gt; Random cone problems of varying sizes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NB: Can use the method described in the long SCS paper to generate these
problems. Will need to take generated problem and form the self-dual
homogeneous embedding in order to recover the feasibility problem, however.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;[\circ]&lt;/script&gt; Open test suite for feasibility problems?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;To the best of my knowledge, none exist!&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Literature Review: QP + Alternating Projections</title>
   <link href="/2016/11/09/qp-alt-proj/"/>
   <updated>2016-11-09T00:00:00-08:00</updated>
   <id>/2016/11/09/qp-alt-proj</id>
   <content type="html">&lt;p&gt;Fukushima (A finitely convergent algorithm for convex inequalities)
– a subgradient method&lt;/p&gt;

&lt;p&gt;Bauschke + Borwein also propose a subgradient method (subgradient to get
outer approximation of set, then project onto it) – feels very similar to what’s
happening here&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Bauschke Survey on Projection Algorithms</title>
   <link href="/2016/11/09/borwein-survey/"/>
   <updated>2016-11-09T00:00:00-08:00</updated>
   <id>/2016/11/09/borwein-survey</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;/papers/Projection Methods and Monotone Operators.pdf&quot;&gt;Source: Bauschke’s Projection Methods and Monotone Operators&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h3&gt;
&lt;p&gt;A mapping &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is a &lt;strong&gt;non-expansive operator&lt;/strong&gt; if it is always true that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\norm{Tx - Ty} \leq \norm{x - y}.&lt;/script&gt;

&lt;p&gt;It is &lt;em&gt;strictly&lt;/em&gt; non-expansive if the above inequality is strict, and it
is an &lt;strong&gt;isometry&lt;/strong&gt; if the relation always holds with equality. The set
of &lt;strong&gt;fixed points&lt;/strong&gt; of a non-expansive mapping &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\fix{T} = \{x \mid x = Tx \}.&lt;/script&gt;

&lt;p&gt;A useful fact is that convex combinations of non-expansive mappings are also
non-expansive. &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is a &lt;strong&gt;firmly non-expansive operator&lt;/strong&gt; if it can be written
as &lt;script type=&quot;math/tex&quot;&gt;T = 1/2I + 1/2N&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt; the identity and &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; a non-expansive operator.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;C&lt;/strong&gt; a closed convex non-empty subset of the larger space &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, the
orthogonal projection operator &lt;script type=&quot;math/tex&quot;&gt;P_C&lt;/script&gt; is firmly non-expansive and, if &lt;script type=&quot;math/tex&quot;&gt;x \in X&lt;/script&gt;,
then &lt;script type=&quot;math/tex&quot;&gt;P_C x \in C&lt;/script&gt; and in particular &lt;script type=&quot;math/tex&quot;&gt;\inner{y - P_C x}{c - P_C x} \leq 0, \;
\forall y \in C&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;A sequence &lt;script type=&quot;math/tex&quot;&gt;(x_n)&lt;/script&gt; converges linearly to its limit &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; with rate
&lt;script type=&quot;math/tex&quot;&gt;\beta \in [0, 1)&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;\exists \alpha \geq 0&lt;/script&gt; such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\norm{x_n - x} \leq \alpha \beta^n, \; \forall n.&lt;/script&gt;

&lt;p&gt;(The above implies that &lt;script type=&quot;math/tex&quot;&gt;\norm{x_n - x}/\norm{x_{n-1} - x} \leq \beta&lt;/script&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;attracting-mappings-monotone-sequences&quot;&gt;2. Attracting mappings, monotone sequences.&lt;/h2&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;T \colon D \rightarrow D&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; a closed convex non-empty set and &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;
non-expansive, is &lt;strong&gt;attracting&lt;/strong&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; if
&lt;script type=&quot;math/tex&quot;&gt;\forall x \in D \setminus F, f \in F&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\norm{Tx - f} &lt; \norm{x - f}. %]]&gt;&lt;/script&gt;

&lt;p&gt;The intuition is that &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; brings every &lt;script type=&quot;math/tex&quot;&gt;x \in D \setminus F&lt;/script&gt; closer to &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;. 
&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is &lt;em&gt;strongly attracting&lt;/em&gt; or &lt;script type=&quot;math/tex&quot;&gt;\kappa&lt;/script&gt;-attracting with respect to &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; if
&lt;script type=&quot;math/tex&quot;&gt;\exists \kappa &gt; 0&lt;/script&gt; such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa\norm{x - Tx}^2 \leq \norm{x-f}^2 - \norm{Tx - f}^2.&lt;/script&gt;

&lt;p&gt;Nice, intuitive things can be said about the composition of attracting
operators. For example, the fixed set of a composition is the intersection
of the fixed sets of each operator, and the composition of attracting operators
is itself attracting. Similarly for convex combinations of attracting operators.&lt;/p&gt;

&lt;p&gt;A sequence &lt;script type=&quot;math/tex&quot;&gt;(x_n)&lt;/script&gt; is &lt;strong&gt;Féjer monotone&lt;/strong&gt; with respect to a closed convex
non-empty set &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\norm{x_{n+1} - c} \leq \norm{x_n - c} \; \forall c \in C, n \geq 0.&lt;/script&gt;

&lt;h3 id=&quot;subgradient-algorithms&quot;&gt;Subgradient Algorithms&lt;/h3&gt;
&lt;p&gt;These are a class of algorithms for when it is hard to project onto the
target convex set. Instead, project onto a first-order approximation of the
set.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ADMM</title>
   <link href="/2016/10/30/admm/"/>
   <updated>2016-10-30T00:00:00-07:00</updated>
   <id>/2016/10/30/admm</id>
   <content type="html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Common characteristics of big data:
    &lt;ul&gt;
      &lt;li&gt;large (billions of examples)&lt;/li&gt;
      &lt;li&gt;high dimensional&lt;/li&gt;
      &lt;li&gt;stored and/or collected in a distributed fashion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\exists&lt;/script&gt; need for rich, scalable algorithms&lt;/li&gt;
  &lt;li&gt;a &lt;em&gt;decomposition-coordination&lt;/em&gt; procedure: solutions to local subproblems are
coordinated to find solution to global problem&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;spiritual-predecessors&quot;&gt;Spiritual Predecessors&lt;/h3&gt;
&lt;p&gt;Dual ascent is gradient descent on the dual problem. For example, for an
equality constrained optimization problem, the Lagrangian is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(x,y)  = f(x) + y^T(Ax - b).&lt;/script&gt;

&lt;p&gt;Then we set &lt;script type=&quot;math/tex&quot;&gt;x^{k+1} = \operatorname{argmin}_x \mathcal{L}(x, y^k)&lt;/script&gt;,
&lt;script type=&quot;math/tex&quot;&gt;y^{k+1} = y^k + \alpha_k (Ax^{k+1} - b)&lt;/script&gt; (note that the second step is
exactly gradient ascent on the dual variable &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;). Dual decomposition is the same
thing for when &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(x,y)&lt;/script&gt; can be decomposed into
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(x_1, x_2, \ldots, x_i, \ldots, x_n, y)&lt;/script&gt;, where the &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; are
subvectors of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. The dual ascent algorithm then consists of broadcast
(broadcast &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; iterate) and gather (gather &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; iterates) steps.&lt;/p&gt;

&lt;p&gt;Augmented Lagrangian methods add a term &lt;script type=&quot;math/tex&quot;&gt;(\rho/2)\pnorm{Ax - b}{2}^2&lt;/script&gt; to the
objective function of the original problem, which often times makes the
objective “nicer” and never changes the optimal solution to the problem (if it
is subject to &lt;script type=&quot;math/tex&quot;&gt;Ax = b&lt;/script&gt;). The problem with augmented Lagrangian methods,
however, is that they are not amenable to parallelization, even if the original
&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt; is separable.&lt;/p&gt;

&lt;p&gt;(Why shift to the dual perspective at all? Optimizing the Lagrangian leads to
algorithms that naturally satisfy the equality constraints imposed upon us.)&lt;/p&gt;

&lt;h3 id=&quot;admm&quot;&gt;ADMM&lt;/h3&gt;

&lt;p&gt;ADMM is structurally similar to the method of multipliers for the augmented
Lagrangian. It is applied to problems with separable objective functions
and separable constraints. For an optimization problem of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\optmin{x, z}{f(x) + g(z)}{Ax + Bz = c}&lt;/script&gt;

&lt;p&gt;we form the augmented Lagrangian&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_\rho(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - c) + \rho/2 \pnorm{Ax + Bz - c}{2}^2,&lt;/script&gt;

&lt;p&gt;and our updates are of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
x^{k+1} &amp;= \argmin{x}{\mathcal{L}_\rho(x, z^k, y^k)} \\
z^{k+1} &amp;= \argmin{z}{\mathcal{L}_\rho(x^{k+1}, z, y^k)} \\
y^{k+1} &amp;= y^k + \rho (Ax^{k+1} + Bz^{k+1} - c).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the third step is exactly gradient ascent on the dual variable &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;.
There are other ways of writing the ADMM steps (e.g., there is a variant called
scaled ADMM), but these other ways are equivalent to the formulation given above.
With ADMM, we have the best of both worlds: we’ve got the distributed nature of
dual decomposition, and we have the smoothness of the augmented Lagrangian.&lt;/p&gt;

&lt;h3 id=&quot;scaled-admm&quot;&gt;Scaled ADMM&lt;/h3&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;r=Ax + Bz - c&lt;/script&gt; be the residual. Then we can write the Lagrangian as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}_\rho(x, z, y) &amp;= f(x) + g(z) + y^Tr + \rho/2 \pnorm{r}{2}^2 \\
y^Tr + \rho/2 \pnorm{r}{2}^2 &amp;= \rho/2\pnorm{r + (1/\rho)y}{2}^2 -
(1/2\rho)\pnorm{y}{2}^2 \\
 &amp;= \rho/2\pnorm{r + u}{2}^2 - \rho/2\pnorm{u}{2}^2, &amp;
 \text{letting } u = (1/\rho) y.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The Lagrangian becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}_\rho(x, z, u) &amp;= f(x) + g(z) + \rho/2\pnorm{r + u}{2}^2 - \rho/2\pnorm{u}{2}^2,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the ADMM update steps become&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
x^{k+1} &amp;= \argmin{x}{f(x) + \rho/2 \pnorm{Ax + Bz^k - c + u^k}{2}^2} \\
z^{k+1} &amp;= \argmin{z}{g(z) + \rho/2 \pnorm{Ax^{k+1} + Bz - c + u^k}{2}^2} \\
u^{k+1} &amp;= u^k + Ax^{k+1} + Bz^{k+1} - c \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the scaled dual update is analogous to the original dual update, multiplied
through by &lt;script type=&quot;math/tex&quot;&gt;1/\rho&lt;/script&gt;. Note that &lt;script type=&quot;math/tex&quot;&gt;u^k&lt;/script&gt; is the running sum of residuals &lt;script type=&quot;math/tex&quot;&gt;u^0 + \sum_{j=1}^{k} r^j&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;convex-feasibility&quot;&gt;Convex Feasibility&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;ADMM for finding a point in the intersection of two sets.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Say we are searching for any &lt;script type=&quot;math/tex&quot;&gt;x \in \mathcal{C} \cap \mathcal{D}&lt;/script&gt;. If &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;
and &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; are the indicator functions for &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}&lt;/script&gt;,
respectively, then the ADMM formulation of the problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\optmin{x,z}{f(x) + g(z)}{x-z = 0}.&lt;/script&gt;

&lt;p&gt;The ADMM update steps become&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\color{blue}{x^{k+1}} &amp;= \argmin{x}{f(x) + \rho/2 \pnorm{x - (z^k - u^k)}{2}^2} \\
&amp;= \color{blue}{\Pi_\mathcal{C}(z^k - u^k)} \\
\color{blue}{z^{k+1}} &amp;= \argmin{z}{g(z) + \rho/2 \pnorm{x^{k+1} - z + u^k}{2}^2} \\
&amp;= \color{blue}{\Pi_\mathcal{D}(x^{k+1} + u^k)} \\
u^{k+1} &amp;= u^k + x^{k+1} - z^{k+1} \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The interpretation of &lt;script type=&quot;math/tex&quot;&gt;\pnorm{x^k - z^k}{2}&lt;/script&gt; is an upper bound on
&lt;script type=&quot;math/tex&quot;&gt;\dist{\mathcal{C}}{\mathcal{D}}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In order to project onto &lt;script type=&quot;math/tex&quot;&gt;\cap_{i}^{N} A_{i}&lt;/script&gt;, we can
apply ADMM with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{C} &amp;= A_1 \times A_2 \times \ldots A_N \\
\mathcal{D} &amp;= \{(x_1, x_2, \ldots, x_N) \in \mathbb{R}^{nN}
\mid x_1 = x_2 = \ldots = x_N\}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
x_i^{k+1} &amp;= \Pi_{\mathcal{A_i}} (z^k - u_i^k) \\
z^{k+1} &amp;= \bar{x}^{k+1} + \bar{u}^k \\
u_i^{k+1} &amp;= u_i^k + x_i^{k+1} - z^{k+1}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Equivalently, noting that &lt;script type=&quot;math/tex&quot;&gt;\bar{u}^{k+1} = \bar{u}^k + \bar{x}^{k+1} - z^{k+1} = 0&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
x_i^{k+1} &amp;= \Pi_{\mathcal{A_i}} (z^k - u_i^k) \\
u_i^{k+1} &amp;= u_i^k + (x_i^{k+1} - \bar{x}^{k+1}).
\end{align*} %]]&gt;&lt;/script&gt;

</content>
 </entry>
 
 <entry>
   <title>The Heavy Ball Method</title>
   <link href="/2016/10/28/heavy-ball/"/>
   <updated>2016-10-28T00:00:00-07:00</updated>
   <id>/2016/10/28/heavy-ball</id>
   <content type="html">&lt;p&gt;The heavy-ball method is a &lt;em&gt;multi-step&lt;/em&gt; iterative method that exploits iterates
prior to the most recent one. It does so by maintaining the
&lt;strong&gt;momentum&lt;/strong&gt; of the previous two iterates. If you imagine
each iterate as a point in the trajectory of a falling ball, then the points
carry with them a momentum or a velocity. The heavy-ball method exploits that
velocity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{(k+1)} = x^{k} - \alpha \nabla f(x^{(k)}) + \beta(x^{(k)} - x^{(k-1)}),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\alpha, \beta \geq 0&lt;/script&gt; are parameters. The heavy ball method is one
way of guarding against zigzagging trajectories in iterative descent and
alternating projection methods. (Scribed from Polyak’s book.)&lt;/p&gt;

&lt;p&gt;I can’t find anything about accelerating alternating projections with the
heavy-ball method, though Escalante’s book &lt;a href=&quot;/texts/Alternating Projection Methods.pdf&quot;&gt;&lt;em&gt;Alternating Projection Methods&lt;/em&gt;&lt;/a&gt;
does describe how to perform a line search when all of the sets are
affine (I imagine the technique extends beyond the affine case).&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Experiments for Alternating Projections</title>
   <link href="/2016/10/28/alternating-projection-experiments/"/>
   <updated>2016-10-28T00:00:00-07:00</updated>
   <id>/2016/10/28/alternating-projection-experiments</id>
   <content type="html">&lt;p&gt;Steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Framework for &lt;strong&gt;generating random cone programs&lt;/strong&gt;, (see extended SCS paper,
look for code) generating the KKT matrix and in particular the affine set
and the cone whose intersection we wish to probe &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;Base class for the &lt;strong&gt;model&lt;/strong&gt;: &lt;strong&gt;feasibility problem&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Base class for &lt;strong&gt;algorithms: iterative (projection)&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Inherited classes, descendants of (3), that &lt;strong&gt;implement various projection
algorithms&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One thing, though: I’d like to, at least initially, use CVXPY in order to
implement the more involved acceleration schemes (for example, the plane search
along the points in the affine set). Actually, I’d like to use CVXPY for &lt;em&gt;all&lt;/em&gt;
of my algorithms, because I don’t actually know how to write algorithms to
project onto convex sets … (!). // Taking advantage of special cone structures.&lt;/p&gt;

&lt;p&gt;Scratch that. I know how to write algorithms to project onto convex sets, or
I vaguely know how to (subgradient descent methods, for example). What I don’t
know, or don’t yet buy (it is important for me to buy this!), is whether it is
often the case that it is easier to project onto &lt;script type=&quot;math/tex&quot;&gt;\mathcal{K}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathcal{A}&lt;/script&gt;
individually rather than onto &lt;script type=&quot;math/tex&quot;&gt;\mathcal{K} \cap \mathcal{A}&lt;/script&gt; directly.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;

      &lt;p&gt;Kind of strange because there’s only (I think) one point in the
intersection of the two sets. The solution, if it exists, is unique.
But I could be wrong. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Professor Boyd [10/24/2016]</title>
   <link href="/2016/10/24/boyd/"/>
   <updated>2016-10-24T00:00:00-07:00</updated>
   <id>/2016/10/24/boyd</id>
   <content type="html">&lt;h2 id=&quot;notes-from-the-meeting&quot;&gt;Notes from the meeting&lt;/h2&gt;
&lt;p&gt;Goal: See if &lt;em&gt;projection methods&lt;/em&gt; can be accelerated by more intelligent
selection of the iterates. In particular, make more use of the information
that we collect along the way during projection: supporting hyperplanes and
their associated halfspaces, and sequences of iterates upon which to do a
line search.&lt;/p&gt;

&lt;h3 id=&quot;projecting-onto-an-affine-set-and-a-convex-cone-is-general&quot;&gt;Projecting onto an affine set and a convex cone is general&lt;/h3&gt;
&lt;p&gt;Consider starting with an &lt;em&gt;affine set and a convex cone&lt;/em&gt;. This is the most
general setting – it generalizes to any intersection of cones. To see this,
say that we wanted to project a vector &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;C = C_1 \cap C_2 \cap \ldots \cap C_r&lt;/script&gt;,
each &lt;script type=&quot;math/tex&quot;&gt;C_i \in \mathbb{R}^n&lt;/script&gt;. Form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\tilde{C} &amp;= C_1 \times C_2 \times \ldots \times C_r \\
\tilde{A} &amp;= \{z \mid z_1 = z_2 = \ldots = z_r \}, &amp; z = [z_1, z_2, \ldots, z_r]^T
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Finding a point in &lt;script type=&quot;math/tex&quot;&gt;\underset{i}\cap C_i&lt;/script&gt; is equivalent to finding a point
in &lt;script type=&quot;math/tex&quot;&gt;\tilde{C} \cap \tilde{A}&lt;/script&gt;. Note that &lt;script type=&quot;math/tex&quot;&gt;\tilde{C}&lt;/script&gt; is a convex cone and
that &lt;script type=&quot;math/tex&quot;&gt;\tilde{A}&lt;/script&gt; is an affine set. To project onto &lt;script type=&quot;math/tex&quot;&gt;\tilde{A}&lt;/script&gt; from
&lt;script type=&quot;math/tex&quot;&gt;\tilde{C}&lt;/script&gt;, we just take the average of each block &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{bmatrix}
x_1 \\
x_2 \\
\ldots \\
x_r
\end{bmatrix} \xrightarrow{\mathcal{P}} C_1 \times C_2 \times \ldots \times C_r \xrightarrow[\text{ average }]{\mathcal{P}} A&lt;/script&gt;

&lt;h3 id=&quot;projections-can-be-cheap-even-for-extremely-high-dimensional-data&quot;&gt;Projections can be cheap, even for extremely high dimensional data&lt;/h3&gt;
&lt;p&gt;Say we have some variable &lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}^{10^7}&lt;/script&gt; and a fixed &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; in the same
space, and say we want &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; to be the projection of &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; onto a polyhedron.
Provided that the polyhedron is defined by a small number of halfpsaces, we can
compute this projection extremely cheaply. To be more concrete, our optimization
problem is of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\optmin{x}{\frac{1}{2}\pnorm{x - z}{2}^2}{Fx \leq g},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;F \in \mathbb{R}^{10 \times 10^7}&lt;/script&gt;. That is, &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; is short and fat. The
fact that &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; is short is precisely what will let us compute the projection
of &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; onto the polyhedron efficiently.&lt;/p&gt;

&lt;p&gt;The Lagrangian of the above problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(x, \gamma) = \frac{1}{2}\pnorm{x - z}{2}^2 + \gamma^T(Fx - g),
\quad \gamma \in \mathbb{R}^{10}.&lt;/script&gt;

&lt;p&gt;Minimizing it in the normal way, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;&amp; \nabla_x \mathcal{L}(x, \gamma) &amp;= 0 \\
&amp;\iff &amp; x - z + F^T\gamma &amp;= 0 \\
&amp;\iff &amp; x &amp;= z - F^T\gamma
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Substituting &lt;script type=&quot;math/tex&quot;&gt;x = z - F^T\gamma&lt;/script&gt; into our objective function eliminates both
&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\pnorm{z - F^T\gamma - z}{2}^2 &amp;= \pnorm{F^T\gamma}{2}^2 \\
&amp;= (\gamma^T F^T)^T(F^T\gamma) \\
&amp;= \gamma^T FF^T \gamma. \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;FF^T \in \mathbb{R}^{10 \times 10}&lt;/script&gt;! Our minimization problem has
become&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\optmin{\gamma}{\frac{1}{2}\gamma^T FF^T \gamma}{Fz -  FF^T\gamma \leq g, \\ &amp;&amp;&amp; \gamma \geq 0.} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can solve the problem easily after paying the overhead to compute &lt;script type=&quot;math/tex&quot;&gt;Fz&lt;/script&gt;
and &lt;script type=&quot;math/tex&quot;&gt;FF^T&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;generating-a-large-random-cone-program&quot;&gt;Generating a large, random cone program.&lt;/h3&gt;
&lt;p&gt;Recall the KKT conditions for the problem in SCS.&lt;/p&gt;

&lt;p&gt;Randomly generate &lt;script type=&quot;math/tex&quot;&gt;z \in_{\mathcal{R}} \mathbb{R}^n&lt;/script&gt;. Generate &lt;script type=&quot;math/tex&quot;&gt;x = \Pi_C(z)&lt;/script&gt;.
Then generate &lt;script type=&quot;math/tex&quot;&gt;s = z - \Pi(z) = \Pi_{C^*}(z)&lt;/script&gt;. We will have &lt;script type=&quot;math/tex&quot;&gt;x^Ts = 0, x \in C,
s \in C^{*}&lt;/script&gt;. From there, enough will be specified to generate the data matrix
&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; and the vector &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;. What’s more, you’ll have the solution &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, too.&lt;/p&gt;

&lt;h3 id=&quot;canonical-sources-and-algorithms-to-read-up-on&quot;&gt;Canonical sources and algorithms to read up on&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/papers/ON PROJECTION ALGORITHMS FOR SOLVING CONVEX FEASIBILITY PROBLEMS.pdf&quot;&gt;Borwein’s survey on alternating projections&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/papers/HeavyBallLinear.pdf&quot;&gt;On the heavy ball method and momentum&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;“FCM Method,” from the 364B notes? Something about cutting planes with
memory=2. I couldn’t find it.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ideas-on-accelerating-alternating-projections&quot;&gt;Ideas on accelerating alternating projections&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Dumbest, simplest idea (that just might work well enough): project onto
the intersection of the supporting halfspaces to form a new iterate.&lt;/li&gt;
  &lt;li&gt;Somewhat smarter: Keep more than 2 halfspaces. (Will this help? Certainly
not in &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^2.&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;Somewhat smarter still: Project onto more the halfspaces, but also do a
line/plane search of the previous &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; iterates, &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; a parameter.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Suggestion (2):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Say we let &lt;script type=&quot;math/tex&quot;&gt;\tilde{F}&lt;/script&gt; be the supporting hyperplanes we collect for
 the convex cone.&lt;/li&gt;
  &lt;li&gt;And let &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; be a 0-1 diagonal matrix that whose &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th diagonal entry
is 1 if and only if we’re picking up the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th equality constraint.&lt;/li&gt;
  &lt;li&gt;Then, at each step, we’re solving&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\optmin{x}{\pnorm{x - z}{2}^2}{ FAx = b, \\ &amp;&amp;&amp; \tilde{F}x \leq \tilde{b} } %]]&gt;&lt;/script&gt;

&lt;p&gt;Key question: Can we solve this efficiently? Will the trick above help?
Will FA become bigger than A? (No.)&lt;/p&gt;

&lt;p&gt;Suggestion (3):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\optmin{\theta}{\dist{\sum_{i} \theta_i a_i}{\mathcal{C}}^2}{\sum_{i} \theta_i = 1,}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;a_i&lt;/script&gt; are a finite number of previous iterates that were on the affine set. That is, we’re doing a
plane search. Can we solve this cheaply? We don’t have to solve it exactly, just
need to do better than projecting the last of the iterates (the most recent one) onto
&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt;, because that’s what we do by default.
We’re assuming we know how to project onto &lt;script type=&quot;math/tex&quot;&gt;\mathcal{C}&lt;/script&gt;.
Is the objective function differentiable? Does the cost still go down if we
do this? Do this, then do the cutting plane strategy, then project onto affine?&lt;/p&gt;

&lt;p&gt;Boyd was under the impression / had the feeling that if we did three “weird” things,
things might just work out well. One or two might not suffice, however.&lt;/p&gt;

&lt;h3 id=&quot;papers-to-bring-up&quot;&gt;Papers to bring up:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/papers/SET INTERSECTION PROBLEMS- SUPPORTING HYPERPLANES AND QUADRATIC PROGRAMMING.pdf&quot;&gt;Set Intersection Problems: Supporting Hyperplanes &amp;amp; Quadratic Programming&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Upshot: superlinear convergence in certain cases&lt;/li&gt;
      &lt;li&gt;Work to be done: Numerical validations on real test suite&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SuperMann: Special case –&amp;gt; find point in intersection of (convex) sets&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;approaches-to-discuss-for-accelerating-alternating-projections&quot;&gt;Approaches to discuss for accelerating alternating projections:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Solve QP + over-project? (Note that projecting onto the intersection of
two halfspaces is a simple cone projection problem.)&lt;/li&gt;
  &lt;li&gt;What if we just initialize our new iterate at the intersection of the
two halfspaces (when finding the intersection of two sets)?&lt;/li&gt;
  &lt;li&gt;Randomness (Mert Pilanci’s work) (choose which halfspaces to use uniformly
at random?)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;questions&quot;&gt;Questions:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Canonical sources?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;action-items&quot;&gt;Action items:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Hooked into / clone the test suite (Mark N…?)&lt;/li&gt;
  &lt;li&gt;Implement vs. theoretical guarantees – what’s the best way to go about
getting a feel for how “good” an algorithm is? A bit of both simultaneously?
    &lt;ul&gt;
      &lt;li&gt;prove convergence at the very least …?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>SCS: Conic Optimization</title>
   <link href="/2016/10/21/scs/"/>
   <updated>2016-10-21T00:00:00-07:00</updated>
   <id>/2016/10/21/scs</id>
   <content type="html">&lt;p&gt;Key Idea: SCS reduces the problem of solving a convex cone program to the
problem of finding a nonzero point in the intersection of a subspace and a
cone.&lt;/p&gt;

&lt;p&gt;Key Idea: Can take many convex problems and convert them into feasibility
problems by forming the KKT system (i.e., system of equations and inequalities
that constitute the KKT optimality conditions)!&lt;/p&gt;

&lt;p&gt;primal-dual pair –&amp;gt; hoomogeneous self-dual embedding = cvx feasability problem
–&amp;gt; solve with ADMM&lt;/p&gt;

&lt;p&gt;nonzero solution to embedding –&amp;gt; solution to original; else certificate of
infeasibility&lt;/p&gt;

&lt;p&gt;Homogeneous self-dual embeddings traditionally used in interior-point methods.&lt;/p&gt;

&lt;h3 id=&quot;primal-dual-pair&quot;&gt;Primal-Dual Pair&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\optmin{x,s}{c^Tx}{Ax + s = b \\ &amp;&amp;&amp; (x, s) \in \mathbb{R}^n \times \mathcal{K},} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\optmax{r, y}{-b^Ty}{-A^Ty + r = c \\ &amp;&amp;&amp; (r, y) \in \{0\}^n \times \mathcal{K}^*,} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;kkt-conditions&quot;&gt;KKT Conditions&lt;/h3&gt;
&lt;p&gt;The KKT conditions are given by&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Ax^* + s^* = b, \quad s^* \in \mathcal{K},&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;A^Ty^* + c = r^*, \quad r^* = 0, \quad y^* \in \mathcal{K}^*, \quad
(y^*)^Ts^* = 0&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The KKT conditions can be embedded into a system of equations and inclusions
to obtain a feasibility problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\begin{bmatrix} r \\ s \\ 0 \end{bmatrix} =
\begin{bmatrix} 0 &amp; A^T \\ -A &amp; 0 \\ c^T &amp; b^T \end{bmatrix}
\begin{bmatrix}x \\ y\end{bmatrix} +
\begin{bmatrix}c \\ b \\ 0 \end{bmatrix}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x, s, r, y) \in
\mathbb{R}^n \times \mathcal{K} \times \{0\}^n \times \mathcal{K}^*,&lt;/script&gt;

&lt;p&gt;where the last equality implicitly encodes complementary slackness and
explicitly enforces the duality gap to be zero.&lt;/p&gt;

&lt;h3 id=&quot;homogeneous-self-dual-embedding&quot;&gt;Homogeneous Self-Dual Embedding&lt;/h3&gt;
&lt;p&gt;The above KKT system has no solution if the original problem is either
primal or dual infeasible. We can get around this problem by adding two
non-negative variables &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\kappa&lt;/script&gt; to the embedding:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\begin{bmatrix} r \\ s \\ \kappa \end{bmatrix} =
\begin{bmatrix} 0 &amp; A^T &amp; c \\ -A &amp; 0 &amp; b \\ -c^T &amp; -b^T &amp; 0\end{bmatrix}
\begin{bmatrix}x \\ y \\ \tau \end{bmatrix}
\end{align*}. %]]&gt;&lt;/script&gt;

&lt;p&gt;A special feature of this problem is that at most of of &lt;script type=&quot;math/tex&quot;&gt;\kappa&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;
are nonzero because &lt;script type=&quot;math/tex&quot;&gt;(x, y, \tau)^T(r, s, \kappa)&lt;/script&gt; equals &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; at every
solution (due to skew symmetry), and each component of the inner product
is non-negative.&lt;/p&gt;

&lt;p&gt;Possible cases:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\tau &gt; 0&lt;/script&gt;: Then the original problem has a solution (&lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt;) is a
scaling factor.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\tau = 0, \kappa &gt; 0&lt;/script&gt;: Then the duality gap is negative (primal or
dual is infeasible).&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\tau = \kappa - 0&lt;/script&gt; provides us with the least information.
The problem can be expressed as&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;u = \begin{bmatrix} x \\ y \\ \tau \end{bmatrix}&lt;/script&gt;,
&lt;script type=&quot;math/tex&quot;&gt;v = \begin{bmatrix} r \\ s \\ \kappa \end{bmatrix}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; be the big matrix
above. Then the modified KKT problem becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\optfind{(u,v)}{v = Qu \\ &amp;&amp;&amp; (u, v) \in \mathcal{C} \times \mathcal{C^*},} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{C} = \mathbb{R}^n \times \mathcal{K}^* \times \mathbb{R}_{+}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Note that the equality constraint can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{bmatrix}Q &amp; -I \end{bmatrix}\begin{bmatrix} u \\ v \end{bmatrix} = 0 %]]&gt;&lt;/script&gt;

&lt;p&gt;Written this way, it becomes evident that the SCS problem can be solved with
the alternating projections method.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Change of Basis</title>
   <link href="/2016/10/21/change-of-basis/"/>
   <updated>2016-10-21T00:00:00-07:00</updated>
   <id>/2016/10/21/change-of-basis</id>
   <content type="html">&lt;p&gt;A coordinate system is a way from translating from numbers to a vector
in Euclidean space.&lt;/p&gt;

&lt;p&gt;Say you have a set of basis vectors &lt;script type=&quot;math/tex&quot;&gt;b_1, b_2&lt;/script&gt; which differ from your basis
vectors &lt;script type=&quot;math/tex&quot;&gt;e_1, e_2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;A matrix &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
B = \begin{bmatrix} b_1 &amp; b_2\end{bmatrix} %]]&gt;&lt;/script&gt; can be thought of as
a transformation that moves &lt;script type=&quot;math/tex&quot;&gt;e_1, e_2&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;b_1, b_2&lt;/script&gt;. B expresses the
actual coordinates of a vector expressed in the foreign basis of &lt;script type=&quot;math/tex&quot;&gt;b_1, b_2&lt;/script&gt;
(from her language to our language). The inverse &lt;script type=&quot;math/tex&quot;&gt;B^{-1}&lt;/script&gt; takes a vector
written in our language to a vector written in her language.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Change of coordinates&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Say &lt;script type=&quot;math/tex&quot;&gt;x = x_1e_1 + x_2e_2 + \ldots + x_ne_n&lt;/script&gt;. The &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; are the coordinates
of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in the standard basis. If &lt;script type=&quot;math/tex&quot;&gt;b_1, \ldots, b_n&lt;/script&gt; is another basis for
&lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^n&lt;/script&gt;, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \tilde{x}_1b_1 + \ldots + \tilde{x}_nb_n,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\tilde{x} = B^{-1}x&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\tilde{x}_i = (B^{-1})_i^Tx_i&lt;/script&gt; is the
first coordinate of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in the &lt;script type=&quot;math/tex&quot;&gt;b_1, \ldots, b_n&lt;/script&gt; basis. This
is apparent because &lt;script type=&quot;math/tex&quot;&gt;x = BB^{-1}x = \tilde{x}_1b_1 + \ldots + \tilde{x}_nb_n&lt;/script&gt;.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;em&gt;Change of coordinates is nice for orthonormal bases&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; be an &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-dimensional vector space with orthonormal basis
&lt;script type=&quot;math/tex&quot;&gt;a_1, a_2, \ldots, a_n&lt;/script&gt;. Note that &lt;script type=&quot;math/tex&quot;&gt;A^{-1} = A&lt;/script&gt;. So the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th coordinate
of a vector &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; in the basis &lt;script type=&quot;math/tex&quot;&gt;\{a_i\}&lt;/script&gt; is just
&lt;script type=&quot;math/tex&quot;&gt;u^Ta_i = \langle u, a_i \rangle&lt;/script&gt;. In particular, this means that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v = \langle v, a_1 \rangle a_1 + \langle v, a_2 \rangle a_2 +
\ldots + \langle v, a_n \rangle a_n.&lt;/script&gt;

&lt;p&gt;That means that &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; is just the sum of its projections onto each of the basis
vectors &lt;script type=&quot;math/tex&quot;&gt;a_i&lt;/script&gt;. Note that both pairwise orthogonality and normality are needed
for this property to hold.&lt;/p&gt;

&lt;p&gt;A similar formulation leads to a natural proof that, if
&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is a subspace of &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;V = U \oplus U^\perp&lt;/script&gt;.
Write &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;v = u + w&lt;/script&gt;. Let &lt;script type=&quot;math/tex&quot;&gt;\{e_1, e_2, \ldots, e_m\}&lt;/script&gt; be an
orthonormal basis for &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;m \leq n&lt;/script&gt;. Choose &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u = \langle v, e_1 \rangle e_1 + \langle v, e_2 \rangle e_2 +
\ldots + \langle v, e_m \rangle e_m&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w = v - u = v - \langle v, e_1 \rangle e_1 + \langle v, e_2 \rangle e_2 +
\ldots + \langle v, e_m \rangle e_m.&lt;/script&gt;

&lt;p&gt;You can verify that &lt;script type=&quot;math/tex&quot;&gt;\langle u, w \rangle = 0&lt;/script&gt;, and that
&lt;script type=&quot;math/tex&quot;&gt;v \in U \cap U^\perp \implies v = 0 \implies U \cap U^\perp = \{0\}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The above construction of &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; is in fact the definition of the orthogonal
projection of &lt;script type=&quot;math/tex&quot;&gt;v \in V&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;. The upshot is that if we have an orthonormal basis &lt;script type=&quot;math/tex&quot;&gt;\{e_i\}&lt;/script&gt;
of a subspace &lt;script type=&quot;math/tex&quot;&gt;U \subseteq V&lt;/script&gt;, then we get, for “free”, the matrix &lt;script type=&quot;math/tex&quot;&gt;P_U&lt;/script&gt; which projects
vectors from &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;. In particular, &lt;script type=&quot;math/tex&quot;&gt;P_U = EE^T&lt;/script&gt;, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E = \begin{bmatrix}
\vert &amp; \vert &amp; \ldots &amp; \vert \\
e_1 &amp; e_2 &amp; \ldots &amp; e_m  \\
\vert &amp; \vert &amp; \ldots &amp; \vert \\
\end{bmatrix}. %]]&gt;&lt;/script&gt;

&lt;p&gt;From this construction we can also see that &lt;script type=&quot;math/tex&quot;&gt;P_U^T = P_U&lt;/script&gt;. Note
that &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt; is &lt;strong&gt;not&lt;/strong&gt; necessarily an orthogonal matrix (in particular, it is not
necessarily square – it’s only square when U and V have the same dimension).
And &lt;script type=&quot;math/tex&quot;&gt;P_U&lt;/script&gt; is not necessarily orthogonal. Indeed, it doesn’t even necessarily 
have orthogonal or orthonormal columns.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Speeding up Subgradient Methods & Alternating Projections</title>
   <link href="/2016/10/20/speeding-up-alt-projections/"/>
   <updated>2016-10-20T00:00:00-07:00</updated>
   <id>/2016/10/20/speeding-up-alt-projections</id>
   <content type="html">&lt;p&gt;Plan of attack (not necessarily sequential):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Get up to speed with Boyd’s notes on subgradient methods + alternating
projections&lt;/li&gt;
  &lt;li&gt;Skim Bertseka’s sections on subgradient methods&lt;/li&gt;
  &lt;li&gt;Skim survey of speedup techniques for alternating projections
(under/overprojection, dyskstra)&lt;/li&gt;
  &lt;li&gt;Skim ideas for QP-acceleration (Pang’s work)&lt;/li&gt;
  &lt;li&gt;Contribute — propose acceleration techniques (non-rigorously at first!)&lt;/li&gt;
  &lt;li&gt;QP “line search”&lt;/li&gt;
  &lt;li&gt;random perturbations &amp;amp; drift directions&lt;/li&gt;
  &lt;li&gt;interpolating between new data and previous estimate&lt;/li&gt;
  &lt;li&gt;sketch system of inequalities implied by collection of supporting
hyperplanes&lt;/li&gt;
  &lt;li&gt;What additional structure do we gain / what structure do we lose by changing
one of the sets to a convex cone? (different types of cones – second order,
exponential, …)&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;dykstras-method-d83d83&quot;&gt;Dykstra’s Method &lt;a href=&quot;/papers/Dykstra - An Algorithm for Restricted Least Squares Regression.pdf&quot;&gt;[D83]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Goal: Find a least squares projection onto a convex set.&lt;/p&gt;

&lt;h3 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h3&gt;
&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; be a cone, and consider the problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}\min_{x \in K} \lVert g - x \rVert.\end{align} \label{min-cone}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;g^*&lt;/script&gt; is optimal if and only if &lt;script type=&quot;math/tex&quot;&gt;\inner{g - g^*}{g^*}=0&lt;/script&gt; (orthogonality)
and &lt;script type=&quot;math/tex&quot;&gt;\inner{g - g^*}{z} \leq 0, \; \forall z \in K&lt;/script&gt; (supporting hyperplane).
Also, &lt;script type=&quot;math/tex&quot;&gt;g - g^*&lt;/script&gt; is the projection of &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;K^\circ&lt;/script&gt; (see my notes on
alternating projections).&lt;/p&gt;

&lt;h3 id=&quot;the-algorithm&quot;&gt;The Algorithm&lt;/h3&gt;

&lt;p&gt;… &lt;a href=&quot;/papers/Dykstra - An Algorithm for Restricted Least Squares Regression.pdf&quot;&gt;[D83]&lt;/a&gt; is old, and the result actually holds for projecting onto
the intersection of general closed convex sets (not necessarily cones). A
natural way to think about the process is that the projections are
“pulled back” before re-projecting. So say we are projecting onto &lt;script type=&quot;math/tex&quot;&gt;C \cap D&lt;/script&gt;,
and say that &lt;script type=&quot;math/tex&quot;&gt;x_0 \in C&lt;/script&gt; (that is, we start off in &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;). Then
&lt;script type=&quot;math/tex&quot;&gt;x_0 \xrightarrow{P_D} y_0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y_0 \xrightarrow{P_C} x_1&lt;/script&gt;. But now,
instead of projecting &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; back onto &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; to form the next &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; iterate,
we instead take &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; and “pull it back” by in the opposite direction &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;
was projected. In particular, we take &lt;script type=&quot;math/tex&quot;&gt;\tilde{x_1} := x_1 + p_0&lt;/script&gt;, where
&lt;script type=&quot;math/tex&quot;&gt;p_1 := \tilde{x_0} - y_0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\tilde{x_0} = x_0&lt;/script&gt;.
We then form &lt;script type=&quot;math/tex&quot;&gt;y_1&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;P_D(\tilde{x_1})&lt;/script&gt;. Similarly,
to get &lt;script type=&quot;math/tex&quot;&gt;x_2&lt;/script&gt;, we form &lt;script type=&quot;math/tex&quot;&gt;\tilde{y_1} := y_1 + q_1&lt;/script&gt;,
where &lt;script type=&quot;math/tex&quot;&gt;q_1 := \tilde{y_0} - x_1&lt;/script&gt;,
&lt;script type=&quot;math/tex&quot;&gt;\tilde{y_0} = y_0&lt;/script&gt;. Then we take &lt;script type=&quot;math/tex&quot;&gt;x_2 = P_C(\tilde{y_1})&lt;/script&gt;, and we continue
this process until convergence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dykstra_algorithm.png&quot; style=&quot;margin:auto&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Research Directions, Week 4</title>
   <link href="/2016/10/17/research-directions/"/>
   <updated>2016-10-17T00:00:00-07:00</updated>
   <id>/2016/10/17/research-directions</id>
   <content type="html">&lt;p&gt;Until now, I’ve spent most of my time reviewing foundations and learning
about classic methods, including subgradient methods, proximal methods,
and localization methods.&lt;/p&gt;

&lt;p&gt;I should begin playing with these methods. That is, I need to take a step
back (or rather, a step forward?) from the literature, and I need to begin
creating and contributing on my own.&lt;/p&gt;

&lt;p&gt;There are two things that are immediate with which I would like to play. They
both fall under the category of variants on localization methods.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first is a variant on the “line search” used in alternating projections,
where the sets &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; are affine and conical, respectively.  Can we
play with how the iterates are chosen? (Like under- and over-projection, for
example.) And will playing with these iterates speed up convergence?&lt;/li&gt;
  &lt;li&gt;The second is a variant on the cutting plane method. Can we be smarter
about which cutting planes we keep and which ones we discard? Can randomness
help us? Can we “sketch” a convex function? Can we represent it in a
lower-dimensional space?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A third project that I am interested in is applying randomized methods to
convex optimization or convex feasibility problems (or even to nonconvex
feasibility problems). Mert Pilanci does work in this area, and he is at
Stanford.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>David Hallac [10/17/2016]</title>
   <link href="/2016/10/17/david-hallac/"/>
   <updated>2016-10-17T00:00:00-07:00</updated>
   <id>/2016/10/17/david-hallac</id>
   <content type="html">&lt;p&gt;Hallac specializes in methods for scalable optimization and
unsupervised learning (time series anomaly detection …).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An interesting idea: Can we find a way of marrying
first-order methods and second-order methods? A system
that tells you whether a first-order method would suffice
for your problem, or whether you require a second-order
method. A pseudo-first-order method that used approximate
Newton steps when required. Along the lines of randomization.&lt;/li&gt;
  &lt;li&gt;Why don’t more people use convex optimization / cvxpy?
According to Hallac, there simply isn’t much awareness about
it. That’s why Boyd travels so often, preaching about DCP
/ cvxpy. Moreover, people don’t have sufficient mathematical
training.
*&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Cones [Foundations]</title>
   <link href="/2016/10/14/cones/"/>
   <updated>2016-10-14T00:00:00-07:00</updated>
   <id>/2016/10/14/cones</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;cone&lt;/strong&gt; is a set &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;x \in K
\implies \theta x \in K, \; \forall \theta \geq 0&lt;/script&gt; &lt;a href=&quot;/texts/Convex Optimization [Boyd].pdf&quot;&gt;[BV04]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; A &lt;strong&gt;convex cone&lt;/strong&gt; is a cone that is also convex:
&lt;script type=&quot;math/tex&quot;&gt;\theta x + (1 - \theta) y \in K&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;x, y \in K&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\theta \geq 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;An immediate corollary is that &lt;script type=&quot;math/tex&quot;&gt;x + y \in K&lt;/script&gt; whenever &lt;script type=&quot;math/tex&quot;&gt;x, y \in K&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Why do we care about cones?&lt;/p&gt;

&lt;p&gt;Cones are “among the simplest convex sets” – think of them as the analog of
subspaces in linear analysis for convex analysis &lt;a href=&quot;/texts/Convex Analysis and Minimization Algorithms I.pdf&quot;&gt;[HU91]&lt;/a&gt;. Note that
a subspace is a convex cone, but a convex cone is not necessarily a subspace.
Indeed, the only thing that prevents a convex cone from being a subspace is
symmetry (it is not necessarily the case that &lt;script type=&quot;math/tex&quot;&gt;K = -K&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;A hierarchy is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Subspaces &lt;script type=&quot;math/tex&quot;&gt;\subsetneq&lt;/script&gt; Convex Cones &lt;script type=&quot;math/tex&quot;&gt;\subsetneq&lt;/script&gt; Convex Sets&lt;/li&gt;
  &lt;li&gt;Subspaces &lt;script type=&quot;math/tex&quot;&gt;\subsetneq&lt;/script&gt; Convex Cones &lt;script type=&quot;math/tex&quot;&gt;\subsetneq&lt;/script&gt; Cones&lt;/li&gt;
  &lt;li&gt;(The leftmost item is the most restrictive, and the rightmost item is the
least restrictive.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the other hand, Cones &lt;script type=&quot;math/tex&quot;&gt;\not\subset&lt;/script&gt; Convex Sets. Neither category contains
the other.&lt;/p&gt;

&lt;p&gt;Examples of convex cones&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;norm cone&lt;/strong&gt; associated with &lt;script type=&quot;math/tex&quot;&gt;\vert\vert \cdot \vert\vert&lt;/script&gt; is
&lt;script type=&quot;math/tex&quot;&gt;\{(x, y) \; \mid \; \vert\vert x \vert\vert \leq t \}&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;The &lt;strong&gt;second-order cone&lt;/strong&gt; is the &lt;script type=&quot;math/tex&quot;&gt;\ell_2&lt;/script&gt;-norm cone&lt;/li&gt;
      &lt;li&gt;The &lt;strong&gt;positive-semidefinite cone&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{S}^n_+ = \{ X \in \mathbf{S}^{n} \mid X \geq 0\}&lt;/script&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A &lt;strong&gt;proper cone&lt;/strong&gt; is a cone that is&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;convex&lt;/li&gt;
  &lt;li&gt;closed&lt;/li&gt;
  &lt;li&gt;solid (non-empty interior)&lt;/li&gt;
  &lt;li&gt;pointed (&lt;script type=&quot;math/tex&quot;&gt;x \in K \implies -x \not\in K \text{ or } x = 0&lt;/script&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another reason we care about cones is that we can use proper cones to define
&lt;strong&gt;generalized inequalities&lt;/strong&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Alternating Projections</title>
   <link href="/2016/10/14/alternating-projections/"/>
   <updated>2016-10-14T00:00:00-07:00</updated>
   <id>/2016/10/14/alternating-projections</id>
   <content type="html">&lt;h2 id=&quot;projections-onto-closed-convex-sets&quot;&gt;Projections onto Closed Convex Sets&lt;/h2&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\norm}[1]{\left\lVert{#1}\right\rVert}
\newcommand{\inner}[2]{\langle{#1}, {#2}\rangle}&lt;/script&gt;A projection onto nonempty
closed convex set &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; is &lt;em&gt;unique&lt;/em&gt; &lt;a href=&quot;/texts/Convex Analysis and Minimization Algorithms I.pdf&quot;&gt;[U91]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1.&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;y_x \in C&lt;/script&gt; is the projection of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; iff&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle x - y_x, y - y_x \rangle \leq 0\; \forall y \in C.&lt;/script&gt;

&lt;p&gt;The geometric interpretation of this is nice — the angle between
&lt;script type=&quot;math/tex&quot;&gt;x - y_x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y - y_x&lt;/script&gt; is obtuse for any &lt;script type=&quot;math/tex&quot;&gt;y \in C&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt; For convex &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;P_C&lt;/script&gt; is &lt;strong&gt;nonexpansive&lt;/strong&gt;: that is,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert P_C(x) - P_C(y) \rVert \leq \lVert x - y \rVert .&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;
We prove that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert P_C(x) - P_C(y)\rVert^2 \leq \langle P_C(x) - P_C(y), x - y \rangle,&lt;/script&gt;

&lt;p&gt;applying Cauchy-Schwarz to which will give the desired result. By theorem 1,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle x - P_C(x), P_C(y) - P_C(x) \rangle \leq 0 \text{ (1)}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle y - P_C(y), P_C(x) - P_C(y) \rangle \leq 0 \text{ (2)}.&lt;/script&gt;

&lt;p&gt;Adding (2) to (1) proves the claim:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\inner{x - P_Cx}{P_C(y) - P_C(x)} + \inner{y - P_C(y)}{P_C(x) - P_C(y)} \leq 0 \\
&amp;\iff  -\inner{x - P_C(x)}{P_C(x) - P_C(y)} + \inner{y - P_C(y)}{P_C(x) - P_C(y)} \leq 0 \\
&amp;\iff  \inner{P_C(x) - P_C(y)}{y - x + P_C(x) - P_C(y)} \leq 0 \\
&amp;\iff  \inner{P_C(x) - P_C(y)}{P_C(x) - P_C(y)} \leq 
  \inner{P_C(x) - P_C(y)}{x - y} \\
&amp;\iff  \pnorm{P_C(x) - P_C(y)}{2}^2 \leq  \inner{P_C(x) - P_C(y)}{x - y}  \\
&amp;\implies  \pnorm{P_C(x) - P_C(y)}{2} \leq  \pnorm{x - y}{2}  \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Q.E.D.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; is a subspace, then &lt;script type=&quot;math/tex&quot;&gt;\pnorm{P_C(x)}{2} = \pnorm{Q^Tx}{2}&lt;/script&gt;
(&lt;script type=&quot;math/tex&quot;&gt;P_C = QQ^T&lt;/script&gt; for some &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; with orthonormal columns).&lt;/p&gt;

&lt;h2 id=&quot;projection-onto-a-closed-convex-cone&quot;&gt;Projection onto a Closed Convex Cone&lt;/h2&gt;
&lt;p&gt;Projecting onto closed convex cones is “nice”, in some sense – these
projections begin to look like projections onto subspaces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;polar cone&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;K^\circ&lt;/script&gt; of a cone &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K^\circ = \left\{s \mid s^Tx \leq 0 \; \forall x \in K\right\}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;dual cone&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;K^*&lt;/script&gt; is defined as &lt;script type=&quot;math/tex&quot;&gt;-K^\circ.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Why care about the polar (dual) cone?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The polar cone is always a closed convex cone.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; a subspace &lt;script type=&quot;math/tex&quot;&gt;\implies K^\circ = K^\bot&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;In this sense, the polar/dual cone generalizes the orthogonal complement&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;K^{\circ\circ}&lt;/script&gt; is the closure of &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;K' \subset K \implies (K')^\circ \supset K^\circ&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem.&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; a closed convex cone, then &lt;script type=&quot;math/tex&quot;&gt;y_x = P_K(x)&lt;/script&gt; iff the
following properties hold:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;y_x \in K&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\; x-y_x \in K^\circ&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\langle x - y_x, y_x \rangle = 0&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Corollaries.&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P_K(x) = 0 \iff x \in K^\circ&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P_K(\alpha x) = \alpha P_K(x) \; \forall \alpha \geq 0&lt;/script&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P_K(-x) = -p_{-K}(x)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x = P_K(x) + P_{K^\circ}(x)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Proof of 4.&lt;/em&gt;
We want to show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y \in K,\; z \in K^\circ,\; x=y+z,\; \inner{y}{z}=0 \implies y=P_K(x), z=P_{K^\circ}(x).&lt;/script&gt;

&lt;p&gt;First observe that &lt;script type=&quot;math/tex&quot;&gt;\forall q \in K, \; \inner{x-y}{q-y} = \inner{z}{q-y} =
\inner{z}{q} \leq 0&lt;/script&gt;, where the last equality comes from the definition of the
polar cone. So &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is the projection of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Next observe that &lt;script type=&quot;math/tex&quot;&gt;\forall q \in K^\circ,\; \inner{x-z}{q-z} = \inner{y}{q-z}
= \inner{y}{q} \leq 0&lt;/script&gt;, where again the last equality comes from the
definition of the polar cone. So &lt;script type=&quot;math/tex&quot;&gt;z = x - P_K(x) = P_{K^\circ}(x)&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;alternating-projections-method&quot;&gt;Alternating Projections Method&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Find a point in the intersection of some closed convex sets &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; and
&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, given &lt;script type=&quot;math/tex&quot;&gt;x_0 \in C&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;: Alternately project onto each set:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_k = P_D(x_k), \quad x_{k+1} = P_C(y_k), \quad k=0, 1, 2&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Caveat&lt;/strong&gt;: Can be slow, but can be sped up when projections are easy to compute.
&lt;strong&gt;Theorem [CG59]&lt;/strong&gt;: If &lt;script type=&quot;math/tex&quot;&gt;C \cap D \neq \emptyset&lt;/script&gt;, then
&lt;script type=&quot;math/tex&quot;&gt;\lim x_k = \lim y_k = x^* \in C \cap D&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; be any point in the intersection. Note that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\lVert x_k - \bar{x} \rVert ^2 &amp;= \lVert x_k - y_k + y_k - \bar{x} \rVert^2 \\
&amp;= \lVert x_k - y_k \rVert^2 + \lVert y_k - \bar{x} \rVert^2 + 2(x_k - y_k)^T(y_k - \bar{x}) \\
&amp;\geq \lVert x_k - y_k \rVert^2 + \lVert y_k - \bar{x} \rVert^2,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the inequality comes from theorem 1. We can then see that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert y_k - \bar{x} \rVert^2
\leq \lVert x_k - \bar{x} \rVert^2 - \lVert y_k - x_k \rVert^2 \quad (3.1)&lt;/script&gt;

&lt;p&gt;and, similarly,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert x_{k+1} - \bar{x} \rVert^2 \leq
\lVert y_k - \bar{x} \rVert^2 - \lVert x_{k+1} - y_k \rVert^2 \quad (3.2).&lt;/script&gt;

&lt;p&gt;which imply that (1) &lt;script type=&quot;math/tex&quot;&gt;y_k&lt;/script&gt; is closer to &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; than &lt;script type=&quot;math/tex&quot;&gt;x_k&lt;/script&gt; is, (2)
&lt;script type=&quot;math/tex&quot;&gt;x_{k+1}&lt;/script&gt; is closer still to &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt;, and (3)
&lt;script type=&quot;math/tex&quot;&gt;\lVert y_k - \bar{x} \rVert&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lVert x_{k+1} - \bar{x} \rVert&lt;/script&gt;
are both &lt;script type=&quot;math/tex&quot;&gt;\leq \lVert x_0 - \bar{x} \rVert&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In particular, &lt;script type=&quot;math/tex&quot;&gt;\{x_k\}&lt;/script&gt; is a bounded sequence and thus has a subsequence
that converges to some limit, call it  &lt;script type=&quot;math/tex&quot;&gt;x^*&lt;/script&gt;. Since &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; is closed by
assumption, &lt;script type=&quot;math/tex&quot;&gt;x^* \in C&lt;/script&gt;. It remains to be shown that &lt;script type=&quot;math/tex&quot;&gt;x^*&lt;/script&gt; is in &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;.
Consider the sequence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\norm{x_0 - \bar{x}},\; \norm{y_0 - \bar{x}},\; \norm{x_1 - \bar{x}},\;
\norm{y_1 - \bar{x}}, … \quad (3.3)&lt;/script&gt;

&lt;p&gt;By (3.1) and (3.2), we see that (3.3) is decreasing but that it is also bounded.
That is, we see that (3.3) is convergent. Referencing (3.1) and (3.2), we see
that &lt;script type=&quot;math/tex&quot;&gt;\norm{y_k - x_k}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\norm{x_{k+1} - y_k}&lt;/script&gt; must converge to &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;.
But since a subsequence of &lt;script type=&quot;math/tex&quot;&gt;\{x_k\}&lt;/script&gt; goes to &lt;script type=&quot;math/tex&quot;&gt;x^*&lt;/script&gt;, a
subsequence of &lt;script type=&quot;math/tex&quot;&gt;\{y_k\}&lt;/script&gt; must do the same, and we can conclude that
&lt;script type=&quot;math/tex&quot;&gt;x^* \in D&lt;/script&gt; as well. (I’m not sure if the previous sentence is correct;
it should probably instead argue that &lt;script type=&quot;math/tex&quot;&gt;\pnorm{x_k - y_k}{2} \rightarrow 0&lt;/script&gt;
together with the fact that a subsequence of &lt;script type=&quot;math/tex&quot;&gt;\{x_k\} \rightarrow x^*&lt;/script&gt;
implies that &lt;script type=&quot;math/tex&quot;&gt;x^* \in D&lt;/script&gt; because each &lt;script type=&quot;math/tex&quot;&gt;y_k \in D&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;Finally, let &lt;script type=&quot;math/tex&quot;&gt;\bar{x} = x^*&lt;/script&gt;. A subsequence of &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; goes to &lt;script type=&quot;math/tex&quot;&gt;x^*&lt;/script&gt;, as does
one of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;. Since (3.3) converges, we conclude that &lt;script type=&quot;math/tex&quot;&gt;(3.3)&lt;/script&gt; goes to zero
and in particular that &lt;script type=&quot;math/tex&quot;&gt;\{x_k\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\{y_k\}&lt;/script&gt; both converge to &lt;script type=&quot;math/tex&quot;&gt;x^*&lt;/script&gt;.
&lt;em&gt;Q.E.D.&lt;/em&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Projections [Foundations]</title>
   <link href="/2016/10/12/projections/"/>
   <updated>2016-10-12T00:00:00-07:00</updated>
   <id>/2016/10/12/projections</id>
   <content type="html">&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\newcommand{\dist}{\textbf{dist}}&lt;/script&gt;From [BV04]&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dist(x_0, C) = \text{inf}\{||x_0 - x|| | x \in C\},&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;x_0 \in \mathbb{R}^n&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; a closed subset of &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^n&lt;/script&gt;,
&lt;script type=&quot;math/tex&quot;&gt;|| \cdot ||&lt;/script&gt; a norm.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition.&lt;/strong&gt; The &lt;strong&gt;projection&lt;/strong&gt; of &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt; is the point &lt;script type=&quot;math/tex&quot;&gt;z \in C&lt;/script&gt;
such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;||z - x_0|| = \dist(x_0, C).&lt;/script&gt;

&lt;p&gt;For closed and convex &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; and a strictly convex norm (for example, the
Euclidean norm), the projection of a point onto &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; is unique.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;projection on &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;P_C&lt;/script&gt; is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_C(x_0) = \text{arg min}\{||x_0 - x|| | x \in C\}.&lt;/script&gt;

&lt;p&gt;More generally, a projection is a linear transformation &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; from a vector
space &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; to itself such that &lt;script type=&quot;math/tex&quot;&gt;P^2 = P&lt;/script&gt;. We call such transformations
&lt;strong&gt;idempotent&lt;/strong&gt;. &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; can be characterized by the following properties:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; is idempotent.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; is the identity operator &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;\operatorname{Im} P&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;V = \operatorname{Im} P \oplus \operatorname{Ker} P&lt;/script&gt;. To see this, take
&lt;script type=&quot;math/tex&quot;&gt;v \in V&lt;/script&gt; and write it as &lt;script type=&quot;math/tex&quot;&gt;v = u + w&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;u = Px&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w = x - Px = (I - P)x&lt;/script&gt;.
Then &lt;script type=&quot;math/tex&quot;&gt;u \in \operatorname{Im} P&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w \in \operatorname{Ker} P&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A projection is &lt;strong&gt;orthogonal&lt;/strong&gt; if its image and kernel are orthogonal
subspaces &lt;a href=&quot;/texts/Linear Algebra Done Right.pdf&quot;&gt;Linear Algebra Done Right&lt;/a&gt; for a good treatment).
More concretely, if &lt;script type=&quot;math/tex&quot;&gt;(e_1, \ldots, e_m)&lt;/script&gt; is an orthonormal basis of &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;,
then for &lt;script type=&quot;math/tex&quot;&gt;v \in V&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{U}(v) = \langle v, e_1 \rangle e_1 + \ldots + \langle v, e_m \rangle e_m&lt;/script&gt;

&lt;p&gt;To see why, observe that if &lt;script type=&quot;math/tex&quot;&gt;v = a_1e_1 + \ldots a_me_m&lt;/script&gt;, then we can
take the inner product of both sides with  &lt;script type=&quot;math/tex&quot;&gt;e_j&lt;/script&gt; to get that
&lt;script type=&quot;math/tex&quot;&gt;a_j = \langle v, e_j&lt;/script&gt;. We say that &lt;script type=&quot;math/tex&quot;&gt;\langle v, e_j&lt;/script&gt; is the &lt;em&gt;coordinate&lt;/em&gt; of
&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; in the direction &lt;script type=&quot;math/tex&quot;&gt;e_j&lt;/script&gt;. The above equation implies that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P = \begin{bmatrix}
\vert &amp; \vert &amp; \ldots &amp; \vert \\
e_1 &amp; e_2 &amp; \ldots &amp; e_m \\
\vert &amp; \vert &amp; \ldots &amp; \vert \\
\end{bmatrix}. %]]&gt;&lt;/script&gt;

&lt;p&gt;An important property of orthogonal projections is that &lt;script type=&quot;math/tex&quot;&gt;P^T = P&lt;/script&gt;. One way to
see this is as follows. Let &lt;script type=&quot;math/tex&quot;&gt;x = x_1 + x_2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y = y_1 + y_2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2&lt;/script&gt;
orthogonal, &lt;script type=&quot;math/tex&quot;&gt;y_1, y_2&lt;/script&gt; orthogonal, &lt;script type=&quot;math/tex&quot;&gt;Px = x_1, Py = y_1&lt;/script&gt;. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\langle x, Py \rangle &amp;= \langle x_1 + x_2, Py \rangle \\
&amp;= \langle Px + x_2, Py \rangle  \\
&amp;= \langle Px, Py \rangle \\
&amp;= \langle Px, Py + y_2 \rangle \\
&amp;= \langle Px, y\rangle \\
&amp;= \langle x, P^Ty\rangle
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;A property of orthogonal projections that is useful for
optimization is that they are &lt;strong&gt;bounded operators&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
||Pv||^2 &amp;= \langle Pv, Pv \rangle = \langle Pv, v \rangle \leq ||Pv|| \cdot ||v|| \\
\implies ||Pv|| &amp;\leq ||v||
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In convex optimization parlance, we call bounded operators &lt;strong&gt;non-expansive
operators&lt;/strong&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Skimming 364b lecture notes</title>
   <link href="/2016/10/12/364b-lecture-notes/"/>
   <updated>2016-10-12T00:00:00-07:00</updated>
   <id>/2016/10/12/364b-lecture-notes</id>
   <content type="html">&lt;script type=&quot;math/tex; mode=display&quot;&gt;\newcommand{\dist}{\textbf{dist}}&lt;/script&gt;

&lt;h2 id=&quot;subgradient-methodslecturenotessubgradmethodnotespdf&quot;&gt;&lt;a href=&quot;/lecture_notes/subgrad_method_notes.pdf&quot;&gt;Subgradient Methods&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Polykak’s step length&lt;/strong&gt; is a neat way of choosing an ‘optimal’ step size for
&lt;a href=&quot;/lecture_notes/subgrad_method_notes.pdf&quot;&gt;subgradient methods&lt;/a&gt;.
The step size is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha_k = \frac{
              f(x^{(k)}) - (f_{\text{best}} - \gamma^k)}
              {||g^{(k)}||_2^2},&lt;/script&gt;

&lt;p&gt;which can be motivated by looking at the key inequality used when analyzing
the convergence of subgradient methods.&lt;/p&gt;

&lt;p&gt;A cool result: &lt;strong&gt;alternating projections&lt;/strong&gt;, the algorithm for finding a point
in the intersection of convex sets, falls out of using the subgradient method
with Polyak’s step length (!!). The objective function for this problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \max\{\dist(x, C_1), \ldots, \dist(x, C_m)\}.&lt;/script&gt;

&lt;p&gt;Noting that &lt;script type=&quot;math/tex&quot;&gt;f^* = 0&lt;/script&gt;, we can derive that &lt;script type=&quot;math/tex&quot;&gt;x^{k+1} = \Pi_{C_j}[x^{(k)}]&lt;/script&gt;,
where &lt;script type=&quot;math/tex&quot;&gt;C_j&lt;/script&gt; is the farthest set from the current point and &lt;script type=&quot;math/tex&quot;&gt;\Pi_C&lt;/script&gt; is the
Euclidean projection onto &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Convex Optimization Algorithms, Berteskas [2009 Supplement]</title>
   <link href="/2016/10/07/optimization-methods-bertsekas/"/>
   <updated>2016-10-07T00:00:00-07:00</updated>
   <id>/2016/10/07/optimization-methods-bertsekas</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;/texts/Convex-Optimization-Algorithms-Supplement.pdf&quot;&gt;Convex Optimization Algorithms&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Most algorithms for minimizing convex &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; over a convex set &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; make use
of at least one of the following techniques:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Iterative descent&lt;/strong&gt;. The sequence &lt;script type=&quot;math/tex&quot;&gt;\{x_k\} \subset X&lt;/script&gt; is such that
&lt;script type=&quot;math/tex&quot;&gt;\{\phi(x_k)\}&lt;/script&gt; is a decreasing sequence.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Approximation&lt;/strong&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\{x_k\}&lt;/script&gt; is obtained by solving at each step
&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; a tractable approximation of the original optimization problem. The
approximation should improve at each step &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post covers …&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;gradient descent methods&lt;/li&gt;
  &lt;li&gt;subgradient methods&lt;/li&gt;
  &lt;li&gt;polyhedral approximation methods&lt;/li&gt;
  &lt;li&gt;proximal and bundle methods&lt;/li&gt;
  &lt;li&gt;interior-point methods&lt;/li&gt;
  &lt;li&gt;primal-dual methods&lt;/li&gt;
  &lt;li&gt;approximate subgradient methods
&lt;!--more--&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gradient-descent-methods&quot;&gt;Gradient Descent Methods&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;of limited use in general optimization theory because they require
differentiability&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;subgradient-methods&quot;&gt;Subgradient Methods&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;bear a striking resemblance to gradient methods&lt;/li&gt;
  &lt;li&gt;in their simplest form (minimizing a convex &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; over convex set &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;) …&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{k+1} := P_X(x_k - \alpha_kg_k)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;P_X&lt;/script&gt; is the projection onto &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha_k&lt;/script&gt; is the learning rate
at &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g_k&lt;/script&gt; is a subgradient.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A strange property: &lt;script type=&quot;math/tex&quot;&gt;f(x_{x+1})&lt;/script&gt; is possibly &lt;script type=&quot;math/tex&quot;&gt;&gt; f(x_k)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;However, we are guaranteed that the distance from the optimal set is reduced.
    &lt;ul&gt;
      &lt;li&gt;Comes from the fact that&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;||P_X(x) - P_X(y)|| \leq ||x-y|| \forall x, y&lt;/script&gt;

&lt;h3 id=&quot;polyhedral-approximation-methods&quot;&gt;Polyhedral Approximation Methods&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Compute subgradient at each iteration&lt;/li&gt;
  &lt;li&gt;Use previously calculated subgradients to construct piecewise linear
approximations of the cost function and/or constraint set.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Outer Linearization: Cutting Plane Methods&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Key idea: A closed convex set is the intersection of its supporting
halfspaces&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{minimize } &amp; F_k(x) \\
\text{subject to } &amp; x \in X \\
&amp;\downarrow \\
&amp;x_{k+1}
\end{align*} %]]&gt;&lt;/script&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;F_k&lt;/script&gt; is a polyhedral approximation of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_k(x) = \operatorname{max}_i\left\{f(x_i) + (x - x_i)^Tg_i\right\}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;{x_i}&lt;/script&gt; is the sequence of points generated thus far and &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt; is the
subgradient computed in step &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;. This is very neat: if you draw a simple
diagram, you’ll see that &lt;script type=&quot;math/tex&quot;&gt;F_k&lt;/script&gt; sketches the boundary of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;. (Try this
sketch for a quadratic &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, for example.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Theorem: &lt;script type=&quot;math/tex&quot;&gt;\lim x_k&lt;/script&gt; is an optimal solution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Limitations:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;instability&lt;/em&gt; – the algorithm can take large steps &lt;em&gt;away&lt;/em&gt; from the optimum,
and in particular &lt;script type=&quot;math/tex&quot;&gt;x_k&lt;/script&gt; might not be a good starting point for &lt;script type=&quot;math/tex&quot;&gt;F_k(x)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;computational tractability&lt;/em&gt; – number of subgradients used to construct
polyhedral approximation increases without bound as &lt;script type=&quot;math/tex&quot;&gt;k \rightarrow \infty&lt;/script&gt;.
The successive optimization problems become more and more expensive to solve.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Convergence can be slow&lt;/em&gt;. proximal methods are aimed at mitigating
instability*&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Central Cutting Plane Methods&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;F_k(x)&lt;/script&gt; defined as before, but …&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x_{k+1}&lt;/script&gt; is the first component of a &lt;strong&gt;central pair&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;(x_{k+1}, w_{k+1})&lt;/script&gt;.
    &lt;ul&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;x_{k+1} \in S_k&lt;/script&gt;:&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
S_k = \{(x, w) \mid x \in X, F_k(x) \leq w \leq \tilde{f}_k\}
\end{align*}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\tilde{f}_k = \min_{i \leq k} f(x_i)&lt;/script&gt; is the best upper bound of
the optimal value found so far.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/central-cutting-plane.png&quot; style=&quot;margin:auto&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the &lt;em&gt;analytic center of S_k&lt;/em&gt; is one choice for the central point.&lt;/li&gt;
  &lt;li&gt;central cutting plane methods are closely related to
interior point methods &lt;span class=&quot;todo&quot;&gt;(?)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span class=&quot;meta-text&quot;&gt;randomized algorithms seem like they could help make
cutting plane methods more computationally tractable. It seems like I’m not
the first to make this observation. Bertsekas published a seminal paper about
using &lt;a href=&quot;/papers/Solving-Convex-Programs-by-Random-Walks.pdf&quot;&gt;random walks for convex optimization&lt;/a&gt;. A more
recent paper looked at a similar problem (&lt;a href=&quot;/papers/A-Randomized-Cutting-Plane-Method.pdf&quot;&gt;A Randomized Cutting Plane Method&lt;/a&gt;).
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inner Linearization: Simplical Decomposition&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;approximate &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; with the convex hull of an expanding finite &lt;script type=&quot;math/tex&quot;&gt;X_k \subset X&lt;/script&gt;
that contains the extreme points of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; plus an arbitrary &lt;script type=&quot;math/tex&quot;&gt;x_o \in X&lt;/script&gt;.
Each x_k gives a cost improvement.&lt;/li&gt;
  &lt;li&gt;for differentiable convex &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and bounded polyhedral &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;useful when:
    &lt;ol&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; non-linear (takes advantage of fact minimizing linear over &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;
is easy.&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; has a large number of extreme points.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{minimize } &amp; \nabla f(x_k)^T(x - x_k)\\
\text{subject to } &amp; x \in X \\
&amp;\downarrow \\
&amp;\tilde{x}_{k+1}, \\
&amp;X_{k+1} = \{\tilde{x}_{k+1}\} \cup X_k \\
&amp;\downarrow \\
\text{minimize } &amp; f(x)\\
\text{subject to } &amp; x \in \operatorname{conv}(X_{k+1}) \\
&amp;\downarrow \\
&amp;x_{k+1} \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Duality of Outer and Inner Linearization&lt;/em&gt;
&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; closed, proper, and convex &lt;script type=&quot;math/tex&quot;&gt;\implies&lt;/script&gt; outer linearization of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is
equivalent to an inner linearization of the conjugate &lt;script type=&quot;math/tex&quot;&gt;f*&lt;/script&gt; and vice versa.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Generalized Polyhedral Approximation&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;todo&quot;&gt;todo&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proximal Center&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Recall a key problem with the cutting plane method: &lt;em&gt;instability&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Proposed solution: penalize deviations from a reference point &lt;script type=&quot;math/tex&quot;&gt;y_k&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{k+1} \in \text{arg min}_{x \in X} \{ F_k(x) + p_k(x) \},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;F_k&lt;/script&gt; is defined as in the cutting plane method and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p_k(x) = \frac{1}{2c_k}||x - y_k||^2, &amp;&amp; c &gt; 0.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;adding a proximal term trades off efficiency (must solve a QP) for stability&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;proximal-and-bundle-methods&quot;&gt;Proximal and Bundle Methods&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Proximal Point Algorithm&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{k+1} \in \text{arg min}_{x \in \mathbb{R}^n} \left\{
f(x) + \frac{1}{2c_k}||x - x_k||^2 \right\}&lt;/script&gt;

&lt;p&gt;&lt;span class=&quot;todo&quot;&gt;Revisit this – geometric intuition is not obvious.
What advantage do we obtain from adding an additional term to our objective
function?&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Let &lt;script type=&quot;math/tex&quot;&gt;\phi_c(z) = \inf_{x \in \mathbb{R}^n}\left\{
f(x) + \frac{1}{2c_k}||x - x_k||^2 \right\}&lt;/script&gt;. Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\inf_{x} f(x) \leq \phi_c(z) \leq f(z)
\end{align*}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Proximal Cutting Plane Method&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Just like the cutting plane method, except minimize &lt;script type=&quot;math/tex&quot;&gt;F_k(x) +&lt;/script&gt; proximal term.&lt;/li&gt;
  &lt;li&gt;Advantage: Increased stability&lt;/li&gt;
  &lt;li&gt;Limitations:
    &lt;ul&gt;
      &lt;li&gt;trade-off in choice of parameter &lt;script type=&quot;math/tex&quot;&gt;c_k&lt;/script&gt;. Large &lt;script type=&quot;math/tex&quot;&gt;c_k \implies&lt;/script&gt; faster
convergence, less stability. Small &lt;script type=&quot;math/tex&quot;&gt;c_k \implies&lt;/script&gt; slower convergence, more
stability.&lt;/li&gt;
      &lt;li&gt;number of subgradients used in polyhedral approximation can grow to be
large (this problem is inherited from the cutting plane method).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Bundle Methods&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the proximal center is not constrained to be &lt;script type=&quot;math/tex&quot;&gt;x_k&lt;/script&gt;, as it is in the
proximal cutting plane method; rather, it can be any &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;i \leq k&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;todo&quot;&gt;todo: revisit.&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;there’s significant policy about how and when to
discard subgradients. could be interesting to look into.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Dual Proximal Point Algorithms&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;todo&quot;&gt;todo&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;interior-point-methods&quot;&gt;Interior Point Methods&lt;/h3&gt;

&lt;p&gt;Optimization problems with inequality constraints:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{minimize } &amp; F_k(x) \\
\text{subject to } &amp; x \in X, &amp;&amp; g_j(x) \leq 0, &amp;&amp; j = 1, \ldots, r, \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the &lt;script type=&quot;math/tex&quot;&gt;g_j&lt;/script&gt; are convex, as is &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; a closed convex set.
Let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
S = \{x \in X \mid g_j(x) &lt; 0\} %]]&gt;&lt;/script&gt;

&lt;p&gt;be the &lt;strong&gt;interior&lt;/strong&gt; of the set defined
by the inequality constraints. &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; is non-empty by assumption.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Key Idea: a &lt;strong&gt;barrier function&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;B(X)&lt;/script&gt; to the cost that is defined on
 &lt;script type=&quot;math/tex&quot;&gt;\operatorname{int} S&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Key Property: &lt;script type=&quot;math/tex&quot;&gt;\lim_{g_j(x) \rightarrow 0-} B(X) = \infty&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;It is in this sense that &lt;script type=&quot;math/tex&quot;&gt;B(X)&lt;/script&gt; is a barrier.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Examples:
    &lt;ul&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;B(X) = -\sum_j \log(-g_j(x))&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;B(X) = -\sum_j \frac{1}{g_j(x)}&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The barrier method adds a parameter sequence &lt;script type=&quot;math/tex&quot;&gt;\{\epsilon_k\}&lt;/script&gt; s.t.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0 &lt; \epsilon_{k+1} &lt; \epsilon_k %]]&gt;&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\lim \epsilon_k = 0&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;At each iteration, must find …&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x_k \in \text{arg }\min_{x \in S} \{f(x) + \epsilon_kB(x)\}
\end{align*}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Can use Newton’s Method to solve for each iterate &lt;script type=&quot;math/tex&quot;&gt;x_k \in S&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;[BV04] Interlude&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Newton’s Method with Equality Constraints&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Goal: Solve&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{minimize } &amp; f(x)
\text{subject to } &amp; Ax = b,\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{minimize } &amp; \frac{1}{2}x^TPx + q^Tx + r\\
\text{subject to } &amp; Ax = b,\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;has optimality conditions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
Ax^* &amp;= b &amp;&amp; Px^* + q + A^T\nu^* = 0, \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;or, more succintly,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\begin{bmatrix}
  P &amp; A^T \\
  A &amp; 0
\end{bmatrix}
\begin{bmatrix}
x^* \\
\nu^*
\end{bmatrix} &amp;=
\begin{bmatrix}
-q \\
b
\end{bmatrix}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In Newton’s Method, we replace the objective function with its second-order
Taylor approximation near x:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x + v) \approx f(x) + \nabla f(x)^Tv + \frac{1}{2}v^T\nabla^2f(x)v&lt;/script&gt;

&lt;p&gt;This gives the KKT conditions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\begin{bmatrix}
  \nabla^2f(x) &amp; A^T \\
   A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x_{nt} \\
w
\end{bmatrix} &amp;=
\begin{bmatrix}
-\nabla f(x) \\
0
\end{bmatrix}
\end{align*}, %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Delta x_{nt}&lt;/script&gt; the optimal solution to the approximated objective,  &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;
the optimal dual variable.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;meta-text&quot;&gt;Why do we need to use the dual variable &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; in the first
place? Because we are not necessarily looking for the global minimum of
our objective function, but we &lt;em&gt;are&lt;/em&gt; looking for the global maximum of the
lagrangian (and, therefore, we can make use of the fact that we need
the gradient of the lagrangian to be zero).&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Infeasible start Newton Method&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If we start with an infeasible &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, then our &lt;script type=&quot;math/tex&quot;&gt;KKT&lt;/script&gt; conditions look almost
exactly the same as before, except now it is possible that &lt;script type=&quot;math/tex&quot;&gt;Ax \neq b:&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\begin{bmatrix}
  \nabla^2f(x) &amp; A^T \\
   A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x_{nt} \\
w
\end{bmatrix} &amp;=
-\begin{bmatrix}
\nabla f(x) \\
Ax - b
\end{bmatrix}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Ax - b&lt;/script&gt; is the &lt;em&gt;residual&lt;/em&gt; vector for the linear inequality constraints. We
can interpret the above equations as a &lt;em&gt;primal-dual&lt;/em&gt; method for the equality
constrained problem. We define the &lt;em&gt;dual residual&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{d}&lt;/script&gt; and
&lt;em&gt;primal residual&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{p}&lt;/script&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
r_d(x,\nu) = \nabla f(x) + A^T\nu, &amp;&amp; r_p(x, \nu) = Ax - b \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We want to &lt;script type=&quot;math/tex&quot;&gt;r_d&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;r_p&lt;/script&gt; to equal zero (remember, they are residuals).
In particular, we want to take a small step &lt;script type=&quot;math/tex&quot;&gt;\Delta y_{pd} = (\Delta x_{pd},
\Delta \nu_{pd})&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;r(y +z) = 0&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is our current estimate
of &lt;script type=&quot;math/tex&quot;&gt;(x^*, \nu^*)&lt;/script&gt;.  Since&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(y+z) \approx r(y) + Dr(y)z,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; is the differential operator, we want to pick &lt;script type=&quot;math/tex&quot;&gt;\Delta y_{pd}&lt;/script&gt;
such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Dr(y)\Delta y_{pd} = -r(y).&lt;/script&gt;

&lt;p&gt;Our optimality conditions become&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\begin{bmatrix}
  \nabla^2f(x) &amp; A^T \\
   A &amp; 0
\end{bmatrix}
\begin{bmatrix}
\Delta x_{pd} \\
\Delta \nu_{pd} \\
\end{bmatrix} &amp;=
-\begin{bmatrix}
r_d \\
p_d
\end{bmatrix}
\end{align*}. %]]&gt;&lt;/script&gt;

&lt;p&gt;If we write &lt;script type=&quot;math/tex&quot;&gt;\nu^+ = \nu + \Delta \nu_{pd}&lt;/script&gt;, then the optimality conditions
look exactly like those for the feasible start Newton’s method, with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta x_{nt} = \Delta x_{pd}, w = \nu^+ = \nu + \Delta \nu_{pd}.&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\therefore&lt;/script&gt;, the infeasible Newton step is the same as the primal part of the
primal-dual step, and the associated dual vector &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; is the primal-dual variable
&lt;script type=&quot;math/tex&quot;&gt;\nu^+&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Disadvantage of infeasible start Netwon method: hard to know whether problem
is wholly infeasible (useful when you know it’s feasible but unsure how to find
a feasible point). Also slow to converge to feasibility.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Inequality constrained minimization&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Problem of interest:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{minimize } &amp; f_0(x)\\
\text{subject to } &amp; f_i(x) \leq 0, \forall i \\
&amp; Ax = b,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Recall the KKT conditions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
Ax^* &amp;= b \\ 
f_i(x^*) &amp;\leq 0, \forall i \\
\lambda^* &amp;\geq 0 \\
\nabla f_0(x^*) + \sum_i \lambda_i^* f_i(x^*) + A^T\nu^* &amp;= 0 \\
\lambda_i^* f_i(x^*) &amp;= 0, \forall i \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Interior-point methods solve the KKT conditions by applying Newton’s method to
a sequence of equality constrained problems / a sequence of modified versions of
the KKT conditions.&lt;/p&gt;

&lt;p&gt;Hierarchy of methods:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Analytical solutions for linear equality constrained quadratic problems.&lt;/li&gt;
  &lt;li&gt;Newton’s method for linear equality constrained problems with twice
differentiable objective functions –&amp;gt; a sequence of (1).&lt;/li&gt;
  &lt;li&gt;Interior-point methods for linear equality &amp;amp; inequality constrained problems
by reducing to a sequence of linear equality constrained problems.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The idea of a &lt;strong&gt;barrier&lt;/strong&gt; is intuitive ~&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/log-barrier.png&quot; style=&quot;margin:auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A point &lt;script type=&quot;math/tex&quot;&gt;x^*(t)&lt;/script&gt; on the &lt;em&gt;central point&lt;/em&gt; is at most &lt;script type=&quot;math/tex&quot;&gt;m/t&lt;/script&gt; suboptimal, &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; the
number of inequality constraints. The barrier method conveniently gives us dual feasible points along the way (see [BV04] for details).&lt;/p&gt;

&lt;p&gt;The barrier method can be extended to cone programming by using the
&lt;em&gt;generalized logarithm&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;end interlude&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Primal-Dual Methods for Linear Programming&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As of ‘09, these were one of the most popular methods for solving linear
programs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;approximate-subgradient-methods&quot;&gt;Approximate Subgradient Methods&lt;/h3&gt;

&lt;p&gt;Using approximate subgradients can lead to computational savings or even
faster convergence.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-Subgradient methods&lt;/em&gt;
An &lt;strong&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-subgradient&lt;/strong&gt; is just like a normal subgradient except it
can be up to &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; below the graph of the function. More formally, &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt;
is an &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-subgradient* of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(z) \geq f(x) + (z-x)^Tg - \epsilon&lt;/script&gt;

&lt;p&gt;Note that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\cap_{\epsilon \downarrow 0} \partial_\epsilon f(x) = \partial f(x).&lt;/script&gt;

&lt;p&gt;Geometrically, &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-subgradient of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; only if the
latter’s epigraph is contained in the positive halfspace carved out by the
hyperplane with normal &lt;script type=&quot;math/tex&quot;&gt;(-g, 1)&lt;/script&gt; that passes through &lt;script type=&quot;math/tex&quot;&gt;(x, f(x) - \epsilon)&lt;/script&gt;.
This can be seen by rearranging the definition to get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(-g, 1)^T(z, f(z)) \geq (-g, 1)(x, f(x) - \epsilon).&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-subgradient methods are nearly identical to subgradient methods,
except the former aim to converge to the $\epsilon$-optimal set, while the
latter converge to the optimal set.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Incremental Subgradient Methods&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Apply to minmization over closed convex set &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; with additive cost function
&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;, where
&lt;script type=&quot;math/tex&quot;&gt;f(x) - \sum_{i=1}^{m} \tilde{f}_i(x),&lt;/script&gt;
each &lt;script type=&quot;math/tex&quot;&gt;\tilde{f}_i&lt;/script&gt; is convex.&lt;/li&gt;
  &lt;li&gt;This is a generalization of stochastic gradient descent.&lt;/li&gt;
  &lt;li&gt;ISM consists of outer iterations, each of which are a cycle of $m$ inner
iterations.&lt;/li&gt;
  &lt;li&gt;Key idea: If &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; (weight vector) has not changed too much during
an iteration, then the subgradient at the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th inner iteration is
approximately equal to the subgradient of &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; (i.e., &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;’s
value at the beginning of the cycle).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Subgradient Methods with Randomization&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In SGD, randomly permuting the data samples can speed convergence and
decrease error&lt;/li&gt;
  &lt;li&gt;&lt;span class=&quot;todo&quot;&gt;TODO: Look into this&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Incremental Proximal Methods&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;like incremental subgradient methods …  &lt;span class=&quot;todo&quot;&gt;todo&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Paper: cvxflow [Wytock]</title>
   <link href="/2016/10/07/cvxflow/"/>
   <updated>2016-10-07T00:00:00-07:00</updated>
   <id>/2016/10/07/cvxflow</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;../papers/cvxflow.pdf&quot;&gt;A New Architecture for Modern Convex Optimization&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;key-contribution&quot;&gt;Key Contribution&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A framework for expressing optimization problems as computation graphs.&lt;/li&gt;
  &lt;li&gt;This framework allows for better scalability and parallelization&lt;/li&gt;
  &lt;li&gt;Graph = composition of linear operators&lt;/li&gt;
  &lt;li&gt;Can implement on top of TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Existing solvers and modeling frameworks do not support GPUs, distributed
computation
    &lt;ul&gt;
      &lt;li&gt;problem #1: interior point methods are compute bound&lt;/li&gt;
      &lt;li&gt;problem #2: interior point methods are network bound&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Want scalability for massive optimization problems&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;How well do first-order methods scale on cvxflow?&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Interior Point v. First Order Methods</title>
   <link href="/2016/10/06/interior-vs-first-order/"/>
   <updated>2016-10-06T16:20:00-07:00</updated>
   <id>/2016/10/06/interior-vs-first-order</id>
   <content type="html">&lt;p&gt;Interior point methods are &lt;em&gt;robust&lt;/em&gt; to ill-conditioned data and converge
quickly in the number of iterations. However, they exhibit roughly &lt;script type=&quot;math/tex&quot;&gt;O(n^2)&lt;/script&gt;
or &lt;script type=&quot;math/tex&quot;&gt;O(n^3)&lt;/script&gt; runtime, &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; the number of variables &lt;span class=&quot;todo&quot;&gt;
(check) &lt;/span&gt; and do not scale out well. The lack of scaling is due largely
to two reasons: the quadratic/cubic runtime and the communication overhead
incurred when computing the Hessian / performing the matrix multiplication.&lt;/p&gt;

&lt;p&gt;First-order methods exhibit good &lt;em&gt;scaling&lt;/em&gt; but they are not necessary robust.
They are also somewhat suboptimal in the sense that they are restricted to
local information, whereas interior point methods use global information&lt;/p&gt;

&lt;p&gt;A goal is to find a happy medium between the two.&lt;/p&gt;

&lt;p&gt;source: &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwj23ODUlMjPAhWFKWMKHZgXBWAQFgghMAA&amp;amp;url=http%3A%2F%2Fwww2.isye.gatech.edu%2F~nemirovs%2FLect_ModConvOpt.pdf&amp;amp;usg=AFQjCNEVuAHbqT6eDYpmldVz-gBqsBe6tA&amp;amp;sig2=vtEiWGCK1BaDqjJtN1zq1g&amp;amp;bvm=bv.134495766,d.cGc&quot;&gt;lectures on modern convex optimization&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Matt Wytock [10/06/16]</title>
   <link href="/2016/10/06/matt-wytock/"/>
   <updated>2016-10-06T16:10:00-07:00</updated>
   <id>/2016/10/06/matt-wytock</id>
   <content type="html">&lt;h2 id=&quot;questions-for-matt-wytock&quot;&gt;Questions for Matt Wytock&lt;/h2&gt;

&lt;h3 id=&quot;career&quot;&gt;Career&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;What motivated you to stay at Google for so long?&lt;/li&gt;
  &lt;li&gt;Why did you leave Google?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt; Got interested in machine learning (smart-ass). At the time,
very few people knew much anything about the theoretical foundations of even
the simplest machine learning methods (for example, SGD applied to logistic regression).
Left Google in 201_, before the neural network / machine learning zeitgeist began.&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Why did you leave Google for a PhD?&lt;/li&gt;
  &lt;li&gt;Why optimization in particular? Esp. at Google, machine learning – deep learning – is king. What drew you to optimization when you first began studying it, and what draws you to it today?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt; Neural networks were not king at the time Wytock left Google.
 His advisor @ CMU was probably the last of Ng’s crop who still focused on traditional
 methods. &lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Where do you see yourself 1, 3, 5 years from now?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt; @ Google? No specific project interests him. Would embed himself
in applied machine learning. Why not Brain? Lots of smart people, delta would be small.
Upside of Google is the resources they provide, the ability to engineer things at
a large scale. &lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;open-problems-in-convex-optimization-today&quot;&gt;Open Problems in (Convex) Optimization Today&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;re: scalability
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;&lt;strong&gt;first-order methods scale&lt;/strong&gt;, but they are not robust (e.g.,
ill-conditioned data can prevent algorithms from reaching any kind of
solution). &lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;&lt;strong&gt;interior point methods are robust&lt;/strong&gt;,
but they do not scale. &lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;the goal: &lt;strong&gt;a robust and scalable algorithm&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;re: efficiency&lt;/li&gt;
  &lt;li&gt;re: theoretical results
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;Convergence bounds for iterations of first-order
methods on specific types of problems. According to Boyd, this work is
not particularly useful.&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;which problems interest you the most?&lt;/li&gt;
  &lt;li&gt;is there a need for a simple, first-order method for optimization that
scales out (parallelizes) well?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;&lt;strong&gt;Yes!&lt;/strong&gt; But keep in mind that many, many, &lt;em&gt;many&lt;/em&gt;
researchers are working on this problem, and they’ve worked on it for many
years.&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;we are seeing somewhat of a renaissance in applying AI/ML to everyday problems. How do you position methodical optimization in relation to that?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt; There is definitely, undeniably value added by the power of
neural networks to learn latent feature representations. The trade-off is that
non-convexity that they introduce. The closest thing to neural networks’ ability to
learn features is kernel methods, and in particular
&lt;a href=&quot;https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf&quot;&gt;Ben Recht’s paper&lt;/a&gt; on    random features for kernel machines. That said, neural networks and machine
learning are not the end-all-be-all. Consider AlphaGo, for example – its
success depended on classical artificial intelligence techniques, like tree search
and reinforcement learning. Neural networks were just one of the many
tools applied to solve the problem. Methodical optimization is another
tool. You could even imagine designing features for optimization problems
using neural networks. &lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cvxflow&quot;&gt;cvxflow&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;rough sense: how well (problem size in number variables, constraints, time / # computers)
would SCS/SuperSCS + TensorFlow scale?&lt;/li&gt;
  &lt;li&gt;what are the bottlenecks, if any, in getting such a system to scale?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;robust + scalable!&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;stephen boyd mentioned &lt;em&gt;“the dream: scalable reliable convex optimization
for anyone.”&lt;/em&gt; How far away are we from that dream, and what needs to get
done in order to make it a reality?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt;Very far. It’s not clear whether the dream will
ever be realized, says Wytock. Need robust + scalable method.&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Who would benefit from materializing that dream? (Energy?)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;epsilon&quot;&gt;epsilon&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ongoing work? or put on hold?
    &lt;ul&gt;
      &lt;li&gt;&lt;span class=&quot;meta-text&quot;&gt; More or less on hold. next step is to actually
test it on problems to see how it holds up.&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;energy&quot;&gt;energy&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;how does the work that you are currently doing (e.g., cvxflow) intersect with your interest in energy management?&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Open Problems re: LPs</title>
   <link href="/2016/09/28/open-problems/"/>
   <updated>2016-09-28T00:00:00-07:00</updated>
   <id>/2016/09/28/open-problems</id>
   <content type="html">&lt;p&gt;Sept 28
Scrawlings
Open problems and recent work&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unsolved problem in computer science:&lt;/li&gt;
  &lt;li&gt;Does linear programming admit a strongly polynomial-time algorithm?&lt;/li&gt;
  &lt;li&gt;(more unsolved problems in computer science)&lt;/li&gt;
  &lt;li&gt;Does LP admit a strongly polynomial-time algorithm?&lt;/li&gt;
  &lt;li&gt;Does LP admit a strongly polynomial algorithm to find a strictly complementary solution?&lt;/li&gt;
  &lt;li&gt;Does LP admit a polynomial algorithm in the real number (unit cost) model of computation?&lt;/li&gt;
  &lt;li&gt;Are there pivot rules which lead to polynomial-time Simplex variants?&lt;/li&gt;
  &lt;li&gt;Do all polytopal graphs have polynomially bounded diameter?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Question: Why does the theory about linear programs seem to be better developed than that about nonlinear programs? Why are people generally more interested interested in linear programs than the greater optimization framework? (Or, at least, why is Wikipedia more interested in LPs than the greater framework?)&lt;/p&gt;

&lt;p&gt;Question: Is there tension between traditional optimization and reinforcement learning?&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Meeting Notes [Boyd, Diamond] [9/26/16]</title>
   <link href="/2016/09/26/meeting/"/>
   <updated>2016-09-26T00:00:00-07:00</updated>
   <id>/2016/09/26/meeting</id>
   <content type="html">&lt;p&gt;Sept 26 2016
Boyd
Diamond
Agrawal&lt;/p&gt;

&lt;p&gt;Investigating alternate methods of solving LPs and SOCPs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cutting plane methods&lt;/li&gt;
  &lt;li&gt;Proximal methods&lt;/li&gt;
  &lt;li&gt;An analog of conjugate gradients?&lt;/li&gt;
  &lt;li&gt;AA^T to solve linear system, A = 10 rows by 10 Million Columns&lt;/li&gt;
  &lt;li&gt;Alternating projections while retaining inequalities (affine and conic subspace)&lt;/li&gt;
  &lt;li&gt;Alternating projections while retaining a limited number of inequalities or sketching the inequalities&lt;/li&gt;
  &lt;li&gt;Alternating projections or some other algorithm that solves a series of QCQPs to solve an SOCP (project onto a polyhedron, or onto a convex hull of inequalities)&lt;/li&gt;
  &lt;li&gt;The catchmark algorithm applied to SOCPs&lt;/li&gt;
  &lt;li&gt;A randomized approach to solving or approximating LPs or QCQPs&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;Ax = b, a_jx = b_j&lt;/script&gt; (random &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;), project &lt;script type=&quot;math/tex&quot;&gt;x_k&lt;/script&gt; onto that hyperplane to get &lt;script type=&quot;math/tex&quot;&gt;x_{k+1}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LBFGS
catchmark (?)&lt;/p&gt;

&lt;p&gt;conditional gradient programs&lt;/p&gt;

&lt;p&gt;Pontus&lt;/p&gt;

&lt;p&gt;non-expansive operator and fixed points&lt;/p&gt;

&lt;p&gt;2nd order methods are metric invariant
first order methods have variable metrics (must find the right metric – norm should probably make subspaces round ish)&lt;/p&gt;

&lt;p&gt;second order cone – diag plus rank 1&lt;/p&gt;

&lt;p&gt;question: log-barrier and self-concordance?&lt;/p&gt;

&lt;p&gt;Theme: Resurrecting methods from the 50’s and applying them to solving SOCPs or LPs in order to (1) better understand the power of these methods and (2) better understand the hard-ness of these optimizing these types of programs.&lt;/p&gt;

&lt;p&gt;NB: A wide body of research into SOCPs has already been conducted.&lt;br /&gt;
Personal interest: I am interested in problems in which trade-offs must be made. In particular, I am currently interested in the trade-off between correctness and computational feasibility, as measured in time and/or space.&lt;/p&gt;

</content>
 </entry>
 

</feed>
